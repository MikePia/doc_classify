{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uw/.venvs/matchnames/lib/python3.10/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:15: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "# import re\n",
    "\n",
    "import spacy\n",
    "import fitz  # PyMuPDF\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "from joblib import dump, load\n",
    "\n",
    "from agents import getRandomAgent\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"classify_errors.log\", level=logging.INFO)\n",
    "\n",
    "\n",
    "def load_from_pickle(pickle_path):\n",
    "    with open(pickle_path, \"rb\") as f:\n",
    "        df = pickle.load(f)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(path):\n",
    "    try:\n",
    "        doc = fitz.open(path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            try:\n",
    "                text += page.get_text()\n",
    "            except Exception as page_error:\n",
    "                print(f\"Error extracting text from page in {path}: {page_error}\")\n",
    "                continue\n",
    "                # Optionally, continue to the next page or log the error\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Error processing file {path}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()  # Enables progress_apply for pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pickle(pickle_df, dataset_path, pickle_path, data_path):\n",
    "    df = pd.read_csv(dataset_path, header=0)\n",
    "    df[\"fname\"] = data_path + df[\"fname\"]\n",
    "\n",
    "    # Filter for new entries not already in pickle_df\n",
    "    new_entries = df[~df[\"fname\"].isin(pickle_df[\"fname\"])]\n",
    "\n",
    "    # Process new entries\n",
    "    if not new_entries.empty:\n",
    "        new_entries[\"cleaned_text\"] = new_entries[\"fname\"].progress_apply(pdf_to_text)\n",
    "        new_entries[\"tokenized_text\"] = new_entries[\"cleaned_text\"].progress_apply(\n",
    "            clean_and_tokenize\n",
    "        )\n",
    "\n",
    "        # Append new entries to the original pickle_df\n",
    "        updated_pickle_df = pd.concat([pickle_df, new_entries], ignore_index=True)\n",
    "    else:\n",
    "        updated_pickle_df = pickle_df  # No new entries to add\n",
    "\n",
    "    # Save updated DataFrame\n",
    "    updated_pickle_df.to_pickle(pickle_path)\n",
    "\n",
    "    return updated_pickle_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")  # Or a larger model as needed\n",
    "\n",
    "\n",
    "def clean_and_tokenize(text, chunk_size=1000000):\n",
    "    \"\"\"\n",
    "    Tokenizes the text using SpaCy, handling long texts by processing in chunks.\n",
    "\n",
    "    :param text: The text to be tokenized.\n",
    "    :param chunk_size: Maximum chunk size in characters.\n",
    "    :return: A string of the lemmatized tokens.\n",
    "    \"\"\"\n",
    "    # Check if the text length exceeds the chunk size\n",
    "    if len(text) > chunk_size:\n",
    "        # Initialize an empty list to store tokens from all chunks\n",
    "        tokens_all_chunks = []\n",
    "\n",
    "        # Process the text in chunks\n",
    "        for start in range(0, len(text), chunk_size):\n",
    "            end = start + chunk_size\n",
    "            # Extract a chunk of text\n",
    "            chunk = text[start:end]\n",
    "            # Process the chunk\n",
    "            doc = nlp(chunk)\n",
    "            # Extract tokens, lemmatize, and filter as before\n",
    "            tokens = [\n",
    "                token.lemma_ for token in doc if token.is_alpha and not token.is_stop\n",
    "            ]\n",
    "            tokens_all_chunks.extend(tokens)\n",
    "\n",
    "        # Combine tokens from all chunks and return\n",
    "        return \" \".join(tokens_all_chunks)\n",
    "    else:\n",
    "        # If text does not exceed the chunk size, process as before\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "        return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize_chunk(chunk):\n",
    "    \"\"\"\n",
    "    Tokenizes a single chunk of text.\n",
    "    \"\"\"\n",
    "    doc = nlp(chunk)\n",
    "    return [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "\n",
    "def batch_tokenize_texts(texts, batch_size=1000, chunk_size=1000000):\n",
    "    \"\"\"\n",
    "    Tokenize a list of texts in batches, handling long texts by processing in chunks.\n",
    "\n",
    "    :param texts: The list of texts to be tokenized.\n",
    "    :param batch_size: Number of texts to process in a single batch.\n",
    "    :param chunk_size: Maximum chunk size in characters for each text.\n",
    "    :return: A list of lists, where each sublist contains the tokens of a text.\n",
    "    \"\"\"\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        # If the text is longer than chunk_size, split it into chunks\n",
    "        if len(text) > chunk_size:\n",
    "            tokens_all_chunks = []\n",
    "            for start in range(0, len(text), chunk_size):\n",
    "                end = start + chunk_size\n",
    "                chunk = text[start:end]\n",
    "                # Tokenize the chunk and extend the list of tokens\n",
    "                tokens_all_chunks.extend(clean_and_tokenize_chunk(chunk))\n",
    "            processed_texts.append(tokens_all_chunks)\n",
    "        else:\n",
    "            # For texts that don't exceed the chunk size, process as usual\n",
    "            tokens = clean_and_tokenize_chunk(text)\n",
    "            processed_texts.append(tokens)\n",
    "\n",
    "    return processed_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to load df from pickle file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_path = \"/uw/invest-data/classify_presentations/data/dataset.csv\"\n",
    "DATA_PATH = \"/dave/presentations/\"\n",
    "\n",
    "dfpickle_path = \"/dave/data/df.pkl\"\n",
    "force = False\n",
    "if not os.path.exists(dfpickle_path) or force:\n",
    "    print(\"Warning, is dave mounted?\")\n",
    "else:\n",
    "    print(\"Going to load df from pickle file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(dfpickle_path) and not force:\n",
    "    df = pd.read_pickle(dfpickle_path)\n",
    "else:\n",
    "    df = pd.read_csv(dataset_path, header=0)\n",
    "    df[\"fname\"] = DATA_PATH + df[\"fname\"]\n",
    "\n",
    "    tqdm.pandas(desc=\"Processing documents\")\n",
    "    df[\"cleaned_text\"] = df[\"fname\"].progress_apply(pdf_to_text)\n",
    "\n",
    "    df[\"tokenized_text\"] = df[\"cleaned_text\"].progress_apply(clean_and_tokenize)\n",
    "\n",
    "# Only necessary when new data is added to the dataset\n",
    "# update_pickle(df, dataset_path, dfpickle_path, DATA_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_aspect_ratio_and_mix_feature(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Error opening PDF {pdf_path}: {e}\")\n",
    "        raise ValueError(\"Error opening PDF\")\n",
    "        return (\n",
    "            [],\n",
    "            0,\n",
    "            False,\n",
    "        )  # Returning False as the third value for no significant change\n",
    "\n",
    "    aspect_ratios = []\n",
    "\n",
    "    page_count = len(doc)\n",
    "\n",
    "    for page in doc:\n",
    "        rect = page.rect\n",
    "        aspect_ratio = rect.width / rect.height\n",
    "        aspect_ratios.append(aspect_ratio)\n",
    "\n",
    "    # Detect significant changes in aspect ratio\n",
    "    change_frequency = calculate_change_frequency(aspect_ratios)\n",
    "\n",
    "    persistent_changes_raw, persistent_changes_frequency = detect_persistent_changes(\n",
    "        aspect_ratios, change_frequency\n",
    "    )\n",
    "    num_changes, changes_significance = detect_significant_change(aspect_ratios)\n",
    "    stats = extract_aspect_ratio_features(aspect_ratios)\n",
    "    categories = categorize_aspect_ratios(aspect_ratios)\n",
    "\n",
    "    return (\n",
    "        aspect_ratios,\n",
    "        page_count,\n",
    "        persistent_changes_raw,\n",
    "        persistent_changes_frequency,\n",
    "        num_changes,\n",
    "        changes_significance,\n",
    "        stats,\n",
    "        categories,\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_change_frequency(aspect_ratios, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of pages exceeding a given aspect ratio change threshold.\n",
    "\n",
    "    :param aspect_ratios: List of aspect ratios for the document's pages.\n",
    "    :param threshold: Threshold for considering a change in aspect ratio significant.\n",
    "    :return: Percentage of page transitions that exceed the change threshold.\n",
    "    \"\"\"\n",
    "    if len(aspect_ratios) < 2:\n",
    "        # If there's only one page or none, there can't be any transitions\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate the absolute percentage change in aspect ratio between consecutive pages\n",
    "    changes = [\n",
    "        abs(aspect_ratios[i] - aspect_ratios[i - 1]) / aspect_ratios[i - 1]\n",
    "        for i in range(1, len(aspect_ratios))\n",
    "    ]\n",
    "\n",
    "    # Count how many changes exceed the threshold\n",
    "    significant_changes = sum(change > threshold for change in changes)\n",
    "\n",
    "    # Calculate the percentage of transitions that are significant\n",
    "    change_frequency = (significant_changes / (len(aspect_ratios) - 1)) * 100\n",
    "\n",
    "    return change_frequency\n",
    "\n",
    "\n",
    "def detect_significant_change(aspect_ratios, change_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Detects the number of significant aspect ratio changes and assesses their significance.\n",
    "\n",
    "    :param aspect_ratios: List of aspect ratios for each page in the document.\n",
    "    :param change_threshold: Threshold for considering a change in aspect ratio significant, based on change_frequency.\n",
    "    :return: A tuple containing the number of significant changes and an aggregate measure of their significance.\n",
    "    \"\"\"\n",
    "    if len(aspect_ratios) < 2:\n",
    "        # No significant changes can be detected in a single-page document\n",
    "        return 0, 0.0\n",
    "\n",
    "    # Calculate percentage changes between consecutive aspect ratios\n",
    "    changes = np.abs(np.diff(aspect_ratios) / aspect_ratios[:-1])\n",
    "\n",
    "    # Determine significant changes based on the threshold\n",
    "    significant_changes_indices = np.where(changes > change_threshold)[0]\n",
    "    num_significant_changes = len(significant_changes_indices)\n",
    "\n",
    "    # Calculate the significance of changes as the sum of changes that exceed the threshold, normalized by the number of changes\n",
    "    if num_significant_changes > 0:\n",
    "        significance_of_changes = (\n",
    "            np.sum(changes[significant_changes_indices]) / num_significant_changes\n",
    "        )\n",
    "    else:\n",
    "        significance_of_changes = 0.0\n",
    "\n",
    "    return num_significant_changes, significance_of_changes\n",
    "\n",
    "\n",
    "def detect_persistent_changes(aspect_ratios, change_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Detects persistent changes in aspect ratios and adjusts the count based on change frequency.\n",
    "\n",
    "    :param aspect_ratios: List of aspect ratios for each page in the document.\n",
    "    :param change_threshold: Threshold for considering a change in aspect ratio significant.\n",
    "    :return: A tuple containing the raw count of persistent changes and the count adjusted by change frequency.\n",
    "    \"\"\"\n",
    "    if len(aspect_ratios) < 2:\n",
    "        # If there's only one page or none, there can't be any transitions\n",
    "        return 0, 0.0\n",
    "\n",
    "    # Calculate the absolute percentage change in aspect ratio between consecutive pages\n",
    "    changes = [\n",
    "        abs(aspect_ratios[i] - aspect_ratios[i - 1]) / aspect_ratios[i - 1]\n",
    "        for i in range(1, len(aspect_ratios))\n",
    "    ]\n",
    "\n",
    "    # Count the number of persistent changes\n",
    "    persistent_changes_raw = 0\n",
    "    current_persistence = 0\n",
    "\n",
    "    for change in changes:\n",
    "        if change > change_threshold:\n",
    "            current_persistence += 1\n",
    "        else:\n",
    "            if (\n",
    "                current_persistence > 1\n",
    "            ):  # Assuming persistence means more than one consecutive change\n",
    "                persistent_changes_raw += 1\n",
    "            current_persistence = 0\n",
    "\n",
    "    # Check if the last sequence of pages ends with persistent changes\n",
    "    if current_persistence > 1:\n",
    "        persistent_changes_raw += 1\n",
    "\n",
    "    # Calculate change_frequency to adjust the persistent_changes_raw\n",
    "    change_frequency = calculate_change_frequency(aspect_ratios, change_threshold)\n",
    "\n",
    "    # Adjusting the raw count by change frequency to get a frequency-informed measure\n",
    "    # This example simply scales the raw count by the change_frequency percentage; other methods could also be applied\n",
    "    persistent_changes_frequency = persistent_changes_raw * (change_frequency / 100)\n",
    "\n",
    "    return persistent_changes_raw, persistent_changes_frequency\n",
    "\n",
    "\n",
    "def extract_aspect_ratio_features(aspect_ratios):\n",
    "    \"\"\"\n",
    "    Extracts statistical features from a list of aspect ratios.\n",
    "\n",
    "    :param aspect_ratios: List of aspect ratios for the document's pages.\n",
    "    :return: Dictionary of statistical features.\n",
    "    \"\"\"\n",
    "    if not aspect_ratios:  # Check if the list is empty\n",
    "        return {\"mean\": 0, \"std\": 0, \"min\": 0, \"max\": 0}\n",
    "\n",
    "    aspect_ratios_array = np.array(aspect_ratios)\n",
    "    return {\n",
    "        \"mean\": np.mean(aspect_ratios_array),\n",
    "        \"std\": np.std(aspect_ratios_array),\n",
    "        \"min\": np.min(aspect_ratios_array),\n",
    "        \"max\": np.max(aspect_ratios_array),\n",
    "    }\n",
    "\n",
    "\n",
    "def categorize_aspect_ratios(aspect_ratios):\n",
    "    \"\"\"\n",
    "    Categorizes aspect ratios as portrait, landscape, or square.\n",
    "\n",
    "    :param aspect_ratios: List of aspect ratios for the document's pages.\n",
    "    :return: List of categories corresponding to each aspect ratio.\n",
    "    \"\"\"\n",
    "    categories = []\n",
    "    for ar in aspect_ratios:\n",
    "        if ar < 0.95:\n",
    "            categories.append(\"portrait\")\n",
    "        elif ar > 1.05:\n",
    "            categories.append(\"landscape\")\n",
    "        else:\n",
    "            categories.append(\"square\")\n",
    "    return categories\n",
    "\n",
    "\n",
    "def calculate_clustering_features(aspect_ratios, n_clusters=3):\n",
    "    \"\"\"\n",
    "    Performs KMeans clustering on aspect ratios and calculates clustering-based features.\n",
    "\n",
    "    :param aspect_ratios: List of aspect ratios for the document's pages.\n",
    "    :param n_clusters: Number of clusters to form, default is 3.\n",
    "    :return: A dictionary with the calculated features: CDC, CTI, CDS, and MCP.\n",
    "    \"\"\"\n",
    "    # Ensure aspect_ratios is a 2D array for KMeans\n",
    "    aspect_ratios = np.array(aspect_ratios).reshape(-1, 1)\n",
    "\n",
    "    # Perform KMeans clustering\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", ConvergenceWarning)\n",
    "        \n",
    "        # Perform KMeans clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(aspect_ratios)\n",
    "    \n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # Calculate Cluster Diversity Count (CDC)\n",
    "    unique_clusters = len(set(labels))\n",
    "\n",
    "    # Calculate Cluster Transition Indicator (CTI)\n",
    "    transitions = sum(1 for i in range(1, len(labels)) if labels[i] != labels[i - 1])\n",
    "\n",
    "    # Calculate Cluster Distribution Spread (CDS) using entropy\n",
    "    cluster_counts = np.bincount(labels, minlength=n_clusters)\n",
    "    proportions = cluster_counts / np.sum(cluster_counts)\n",
    "    spread = entropy(proportions)\n",
    "\n",
    "    # Calculate Majority Cluster Proportion (MCP)\n",
    "    majority_cluster_proportion = max(cluster_counts) / np.sum(cluster_counts)\n",
    "\n",
    "    # Return the calculated features\n",
    "    return {\n",
    "        \"CDC\": unique_clusters,\n",
    "        \"CTI\": transitions,\n",
    "        \"CDS\": spread,\n",
    "        \"MCP\": majority_cluster_proportion,\n",
    "    }\n",
    "\n",
    "\n",
    "def detect_outliers_z_score(aspect_ratios, threshold=2):\n",
    "    aspect_ratios = np.array(aspect_ratios).flatten()  # Ensures aspect_ratios is 1D\n",
    "    mean_ar = np.mean(aspect_ratios)\n",
    "    std_ar = np.std(aspect_ratios)\n",
    "    outliers = [\n",
    "        i\n",
    "        for i, ar in enumerate(aspect_ratios)\n",
    "        if abs((ar - mean_ar) / std_ar) > threshold\n",
    "    ]\n",
    "\n",
    "    return outliers\n",
    "\n",
    "\n",
    "np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "\n",
    "\n",
    "def calculate_text_density(pdf_path):\n",
    "    \"\"\"\n",
    "    Calculates text density (words per page) for a PDF document.\n",
    "\n",
    "    :param pdf_path: Path to the PDF document.\n",
    "    :return: List of text densities for each page.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_densities = []\n",
    "    for page in doc:\n",
    "        text = page.get_text(\"text\")\n",
    "        word_count = len(text.split())\n",
    "        area = page.rect.width * page.rect.height\n",
    "        text_density = word_count / area if area else 0\n",
    "        text_densities.append(text_density)\n",
    "    return text_densities\n",
    "\n",
    "\n",
    "def correlate_text_density_aspect_ratio(aspect_ratios, text_densities):\n",
    "    \"\"\"\n",
    "    Calculates the Pearson correlation coefficient between aspect ratios and text densities of a document's pages.\n",
    "\n",
    "    :param aspect_ratios: List of aspect ratios for each page.\n",
    "    :param text_densities: List of text densities for each page.\n",
    "    :return: Pearson correlation coefficient, or NaN if the calculation is not possible.\n",
    "    \"\"\"\n",
    "    if len(aspect_ratios) != len(text_densities) or len(aspect_ratios) < 2:\n",
    "        return np.nan  # Ensures there are enough data points and both lists match\n",
    "\n",
    "    return np.corrcoef(aspect_ratios, text_densities)[0, 1]\n",
    "\n",
    "\n",
    "def calculate_text_density_variability(text_densities):\n",
    "    return np.std(text_densities)\n",
    "\n",
    "\n",
    "def calculate_text_density_by_position(text_densities):\n",
    "    \"\"\"\n",
    "    Calculates the average text density for the beginning, middle, and end sections of a document.\n",
    "\n",
    "    :param text_densities: List of text densities for each page.\n",
    "    :return: A tuple with average text densities for the beginning, middle, and end of the document.\n",
    "    \"\"\"\n",
    "    third = len(text_densities) // 3\n",
    "    if third == 0:\n",
    "        return (0, 0, 0)  # Avoid division by zero for very short documents\n",
    "\n",
    "    beginning = np.mean(text_densities[:third])\n",
    "    middle = np.mean(text_densities[third : 2 * third])\n",
    "    end = np.mean(text_densities[2 * third :])\n",
    "\n",
    "    return beginning, middle, end\n",
    "\n",
    "\n",
    "# Function to check for keyword presence\n",
    "def check_keywords(text, keyword_list):\n",
    "    text = text.lower()\n",
    "    return int(any(keyword in text for keyword in keyword_list))\n",
    "\n",
    "\n",
    "def combine_tfidf_keyword(df):\n",
    "    # Step 2: TF-IDF Calculation\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=5000\n",
    "    )  # Adjust number of features as needed\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"tokenized_text\"])\n",
    "\n",
    "    # Step 3. Combine keyword and tfidf features into a single matrix\n",
    "    # Convert binary keyword matches to a matrix\n",
    "    keyword_features = df[[col for col in df.columns if \"_keyword\" in col]].to_numpy()\n",
    "    # Combine TF-IDF features with keyword binary indicators\n",
    "    combined_features = np.hstack((tfidf_matrix.toarray(), keyword_features))\n",
    "\n",
    "    # Now `combined_features` is ready for model training, and should be aligned with your labels.\n",
    "    return combined_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tfidf_keyword_additional_features(df, vectorizer=None):\n",
    "    # Step 2: TF-IDF Calculation\n",
    "    if vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer(max_features=5000)    \n",
    "        tfidf_matrix = vectorizer.fit_transform(df[\"tokenized_text\"])\n",
    "    else:\n",
    "        tfidf_matrix = vectorizer.transform(df[\"tokenized_text\"]) \n",
    "    \n",
    "    # Convert binary keyword matches to a matrix\n",
    "    keyword_features = df[[col for col in df.columns if \"_keyword\" in col]].to_numpy()\n",
    "    \n",
    "    # Assuming new features are already in df and are numeric\n",
    "    additional_features_columns = [\n",
    "        \"aspect_ratio_means\", \"aspect_ratio_std\", \"aspect_ratio_min\", \"aspect_ratio_max\",\n",
    "        \"page_counts\", \"persistent_changes_raw\", \"persistent_changes_frequency\", \"num_changes\",\n",
    "        \"changes_significance\", \"text_density_means\", \"text_density_correlations\", \"text_density_variability\",\n",
    "        \"text_density_beginning\", \"text_density_middle\", \"text_density_end\", \"outliers_counts\",\n",
    "        \"unique_cluster_lists\", \"cluster_transitions_lists\", \"cluster_spreads_lists\",\n",
    "        \"majority_cluster_proportions\", \"portrait_count\", \"landscape_count\", \"square_count\"\n",
    "    ]\n",
    "    additional_features = df[additional_features_columns].to_numpy()\n",
    "    \n",
    "    # Combine TF-IDF features with keyword binary indicators and the additional features\n",
    "    combined_features = np.hstack((tfidf_matrix.toarray(), keyword_features, additional_features))\n",
    "    \n",
    "    return combined_features, vectorizer\n",
    "\n",
    "\n",
    "\n",
    "def append_data_or_nan(a_list, data):\n",
    "    try:\n",
    "        a_list.append(data)\n",
    "    except Exception as e:\n",
    "        logging.info(e)\n",
    "        a_list.append(np.nan)\n",
    "\n",
    "\n",
    "def extract_specific_features(df):\n",
    "    # Initialize empty lists to store your new features\n",
    "    aspect_ratio_means = []\n",
    "    aspect_ratio_std = []\n",
    "    aspect_ratio_min = []\n",
    "    aspect_ratio_max = []\n",
    "    page_counts = []\n",
    "    persistent_changes_raw_list = []\n",
    "    persistent_changes_frequency_list = []\n",
    "    num_changes_list = []\n",
    "    changes_significance_list = []\n",
    "    text_density_means = []\n",
    "    text_density_correlations = []\n",
    "    text_density_variability_lists = []\n",
    "    text_density_beginning_lists = []\n",
    "    text_density_middle_lists = []\n",
    "    text_density_end_lists = []\n",
    "    categories_lists = []\n",
    "    portrait_counts = []\n",
    "    landscape_counts = []\n",
    "    square_counts = []\n",
    "    outliers_counts = []\n",
    "    unique_cluster_lists = []\n",
    "    cluster_transitions_lists = []\n",
    "    cluster_spreads_lists = []\n",
    "    majority_cluster_proportions = []\n",
    "\n",
    "    for pdf_path in tqdm(df[\"fname\"], desc=\"Processing PDFs\"):\n",
    "        try:\n",
    "            logging.info(f\"Processing aspect stuff for {pdf_path}\")\n",
    "            # Run your feature extraction functions\n",
    "            (\n",
    "                aspect_ratios,\n",
    "                page_count,\n",
    "                persistent_changes_raw,\n",
    "                persistent_changes_frequency,\n",
    "                num_changes,\n",
    "                changes_significance,\n",
    "                stats,\n",
    "                categories,\n",
    "            ) = check_aspect_ratio_and_mix_feature(pdf_path)\n",
    "\n",
    "            text_densities = calculate_text_density(pdf_path)\n",
    "            correlation = correlate_text_density_aspect_ratio(\n",
    "                aspect_ratios, text_densities\n",
    "            )\n",
    "            text_density_variability = calculate_text_density_variability(\n",
    "                text_densities\n",
    "            )\n",
    "            (text_density_beginning, text_density_middle, text_density_end) = (\n",
    "                calculate_text_density_by_position(text_densities)\n",
    "            )\n",
    "\n",
    "            outliers = detect_outliers_z_score(aspect_ratios)\n",
    "            clusters = calculate_clustering_features(aspect_ratios)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {pdf_path}: {e}\")\n",
    "\n",
    "            # For simplicity, let's just use some of the features as examples\n",
    "        append_data_or_nan(aspect_ratio_means, stats[\"mean\"])\n",
    "        append_data_or_nan(aspect_ratio_std, stats[\"std\"])\n",
    "        append_data_or_nan(aspect_ratio_min, stats[\"min\"])\n",
    "        append_data_or_nan(aspect_ratio_max, stats[\"max\"])\n",
    "        append_data_or_nan(page_counts, page_count)\n",
    "        append_data_or_nan(persistent_changes_raw_list, persistent_changes_raw)\n",
    "        append_data_or_nan(\n",
    "            persistent_changes_frequency_list, persistent_changes_frequency\n",
    "        )\n",
    "        append_data_or_nan(num_changes_list, num_changes)\n",
    "        append_data_or_nan(changes_significance_list, changes_significance)\n",
    "\n",
    "        append_data_or_nan(text_density_means, np.mean(text_densities))\n",
    "        append_data_or_nan(text_density_correlations, correlation)\n",
    "        append_data_or_nan(text_density_variability_lists, text_density_variability)\n",
    "        append_data_or_nan(text_density_beginning_lists, text_density_beginning)\n",
    "        append_data_or_nan(text_density_middle_lists, text_density_middle)\n",
    "        append_data_or_nan(text_density_end_lists, text_density_end)\n",
    "\n",
    "        append_data_or_nan(\n",
    "            categories_lists, categories\n",
    "        )  # This one is a bit tricky as it's a list. Might aggregate or process further.\n",
    "\n",
    "        append_data_or_nan(\n",
    "            outliers_counts, len(outliers)\n",
    "        )  # Assuming aspect ratios are recalculated within the function\n",
    "        append_data_or_nan(unique_cluster_lists, clusters[\"CDC\"])\n",
    "        append_data_or_nan(cluster_transitions_lists, clusters[\"CTI\"])\n",
    "        append_data_or_nan(cluster_spreads_lists, clusters[\"CDS\"])\n",
    "        append_data_or_nan(majority_cluster_proportions, clusters[\"MCP\"])\n",
    "\n",
    "    portrait_counts = [\n",
    "        cats.count(\"portrait\") if isinstance(cats, list) else 0\n",
    "        for cats in categories_lists\n",
    "    ]\n",
    "    landscape_counts = [\n",
    "        cats.count(\"landscape\") if isinstance(cats, list) else 0\n",
    "        for cats in categories_lists\n",
    "    ]\n",
    "    square_counts = [\n",
    "        cats.count(\"square\") if isinstance(cats, list) else 0\n",
    "        for cats in categories_lists\n",
    "    ]\n",
    "\n",
    "    # Now add these lists as columns to your DataFrame\n",
    "    df[\"aspect_ratio_means\"] = aspect_ratio_means\n",
    "    df[\"aspect_ratio_std\"] = aspect_ratio_std\n",
    "    df[\"aspect_ratio_min\"] = aspect_ratio_min\n",
    "    df[\"aspect_ratio_max\"] = aspect_ratio_max\n",
    "    df[\"page_counts\"] = page_counts\n",
    "    df[\"persistent_changes_raw\"] = persistent_changes_raw_list\n",
    "    df[\"persistent_changes_frequency\"] = persistent_changes_frequency_list\n",
    "    df[\"num_changes\"] = num_changes_list\n",
    "    df[\"changes_significance\"] = changes_significance_list\n",
    "    df[\"text_density_means\"] = text_density_means\n",
    "    df[\"text_density_correlations\"] = text_density_correlations\n",
    "    df[\"text_density_variability\"] = text_density_variability_lists\n",
    "    df[\"text_density_beginning\"] = text_density_beginning_lists\n",
    "    df[\"text_density_middle\"] = text_density_middle_lists\n",
    "    df[\"text_density_end\"] = text_density_end_lists\n",
    "    df[\"outliers_counts\"] = outliers_counts\n",
    "    df[\"unique_cluster_lists\"] = unique_cluster_lists\n",
    "    df[\"cluster_transitions_lists\"] = cluster_transitions_lists\n",
    "    df[\"cluster_spreads_lists\"] = cluster_spreads_lists\n",
    "    df[\"majority_cluster_proportions\"] = majority_cluster_proportions\n",
    "    df[\"portrait_count\"] = portrait_counts\n",
    "    df[\"landscape_count\"] = landscape_counts\n",
    "    df[\"square_count\"] = square_counts\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "keywords = {\n",
    "    \"financial_terms\": [\n",
    "        \"financial\",\n",
    "        \"investment\",\n",
    "        \"share price\",\n",
    "        \"financial metrics\",\n",
    "        \"investment strategy\",\n",
    "    ],\n",
    "    \"legal_statements\": [\n",
    "        \"confidentiality statement\",\n",
    "        \"legal disclaimer\",\n",
    "        \"disclosure statement\",\n",
    "        \"proprietary information\",\n",
    "        \"intellectual property\",\n",
    "    ],\n",
    "    \"company_info\": [\n",
    "        \"company overview\",\n",
    "        \"company analysis\",\n",
    "        \"business model\",\n",
    "        \"company performance\",\n",
    "    ],\n",
    "    \"presentation_content\": [\n",
    "        \"visual aids\",\n",
    "        \"data charts\",\n",
    "        \"case studies\",\n",
    "        \"comparative analysis\",\n",
    "    ],\n",
    "    \"company_targets\": [\"sales targets\", \"company targets\", \"performance targets\"],\n",
    "    \"financial_discussions\": [\n",
    "        \"financial figures\",\n",
    "        \"financial projections\",\n",
    "        \"financial results\",\n",
    "        \"financial language\",\n",
    "    ],\n",
    "    \"regulatory_references\": [\n",
    "        \"SEC filings\",\n",
    "        \"regulatory filings\",\n",
    "        \"external entities\",\n",
    "        \"lawsuits\",\n",
    "    ],\n",
    "    \"detail_descriptions\": [\n",
    "        \"loan details\",\n",
    "        \"product details\",\n",
    "        \"research and development\",\n",
    "        \"financial details\",\n",
    "    ],\n",
    "    \"company_specific\": [\n",
    "        \"company specific\",\n",
    "        \"industry specific\",\n",
    "        \"company-specific analysis\",\n",
    "        \"specific company focus\",\n",
    "    ],\n",
    "    # \"Other Clusters\" category is omitted since it's broad and without specific keywords\n",
    "}\n",
    "\n",
    "\n",
    "def load_df_from_pickle(path):\n",
    "    df = pd.read_pickle(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_np_array_from_pickle(path):\n",
    "    np_array = np.load(path)\n",
    "    return np_array\n",
    "\n",
    "def load_vectorizer(path):\n",
    "    vectorizer = pickle.load(open(path, 'rb'))\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading PreProcessing df from disk\n"
     ]
    }
   ],
   "source": [
    "dfpickle_path = \"/dave/data/df.pkl\"\n",
    "if not os.path.exists(dfpickle_path):\n",
    "    print(\"Go back and do the preprocessing step above before running this cell for feature extraction\")\n",
    "else:\n",
    "    print(\"loading PreProcessing df from disk\")\n",
    "    df = load_from_pickle(dfpickle_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataframe with features,features numpy array, and tdif vectorizer from disk)\n"
     ]
    }
   ],
   "source": [
    "dff_pickle_path = \"/dave/data/df_features.pkl\"\n",
    "features_path = \"/dave/data/features_array.pkl.npy\"\n",
    "features_array_ppath = '/dave/data/features_array.pkl'\n",
    "tdif_vectorizer_pickle_path = '/dave/data/tdif_vectorizer.pkl'\n",
    "\n",
    "force = False\n",
    "if os.path.exists(features_path) and os.path.exists(dff_pickle_path) and os.path.exists(tdif_vectorizer_pickle_path) and not force:\n",
    "    print(\"loading dataframe with features,features numpy array, and tdif vectorizer from disk)\")\n",
    "    features = load_np_array_from_pickle(features_path)\n",
    "    df = load_df_from_pickle(dff_pickle_path)\n",
    "    tdif_vectorizer = load_vectorizer(tdif_vectorizer_pickle_path)\n",
    "else:\n",
    "    df = extract_specific_features(df)\n",
    "    # Apply keyword matching\n",
    "    for category, keyword_list in keywords.items():\n",
    "        df[category + \"_keyword\"] = df[\"tokenized_text\"].apply(\n",
    "            check_keywords, args=(keyword_list,)\n",
    "        )\n",
    "    features, tdif_vectorizer = combine_tfidf_keyword_additional_features(df)\n",
    "    df.to_pickle(dff_pickle_path)\n",
    "    np.save(features_array_ppath, features)\n",
    "    pickle.dump(tdif_vectorizer, open(tdif_vectorizer_pickle_path, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, labels, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def train_model_random_forest(X_train, y_train, X_test, y_test):\n",
    "    # Initialize the RandomForest Classifier\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating random forest model...\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    return rf_classifier\n",
    "\n",
    "\n",
    "def train_model_HistGradientBoosting(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Trains a HistGradientBoostingClassifier model and evaluates it.\n",
    "\n",
    "    :param X_train: Training feature matrix\n",
    "    :param y_train: Training target array\n",
    "    :param X_test: Test feature matrix\n",
    "    :param y_test: Test target array\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the HistGradientBoostingClassifier\n",
    "    hgb_classifier = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    hgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = hgb_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating HistGradientBoostingClassifier model...\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    return hgb_classifier\n",
    "\n",
    "\n",
    "def train_catboost(X_train, y_train, X_test, y_test):\n",
    "    # Initialize the CatBoost Classifier\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=1000,\n",
    "        learning_rate=0.1,\n",
    "        depth=6,\n",
    "        verbose=200,  # It prints the training log every 200 iterations\n",
    "        random_state=42,\n",
    "        eval_metric=\"Accuracy\",  # You can change this to other metrics relevant to your task\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, eval_set=(X_test, y_test), use_best_model=True)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating CatBoostClassifier model...\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_xgboost(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Adjust labels if they start from 1 instead of 0\n",
    "    y_train = y_train - 1\n",
    "    y_test = y_test - 1\n",
    "\n",
    "    # Convert the datasets to DMatrix, which is a high-performance XGBoost data structure\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    # Set up the parameters for XGBoost\n",
    "    params = {\n",
    "        \"max_depth\": 6,\n",
    "        \"eta\": 0.3,\n",
    "        \"objective\": \"multi:softmax\",  # Use softmax for multi-class classification\n",
    "        \"num_class\": 3,  # Specify the number of unique classes\n",
    "        \"eval_metric\": \"mlogloss\",  # Multiclass logloss for evaluation\n",
    "    }\n",
    "    num_rounds = 100\n",
    "\n",
    "    # Train the model\n",
    "    eval_set = [(dtrain, \"train\"), (dtest, \"test\")]\n",
    "    bst = xgb.train(\n",
    "        params, dtrain, num_rounds, evals=eval_set, early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = bst.predict(dtest)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating XGBoost model...\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    return bst\n",
    "\n",
    "\n",
    "def store_model(model, path, type):\n",
    "    if type == \"forest\" or type == \"hgb\":\n",
    "        dump(model, path)\n",
    "    if type == \"catboost\":\n",
    "        model.save_model(path)\n",
    "    elif type == \"xgboost\":\n",
    "        model.save_model(path)\n",
    "        \n",
    "def load_model(path, type):\n",
    "    if type == \"forest\" or type == \"hgb\":\n",
    "        model = load(path)\n",
    "    elif type == \"catboost\":\n",
    "        model = CatBoostClassifier()\n",
    "        model.load_model(path)\n",
    "    elif type == \"xgboost\":\n",
    "        model = xgb.Booster()\n",
    "        model.load_model(path)\n",
    "    return model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating random forest model...\n",
      "Accuracy: 0.9478827361563518\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.90      0.92       135\n",
      "           2       0.93      0.99      0.96       345\n",
      "           3       0.99      0.88      0.93       134\n",
      "\n",
      "    accuracy                           0.95       614\n",
      "   macro avg       0.96      0.92      0.94       614\n",
      "weighted avg       0.95      0.95      0.95       614\n",
      "\n",
      "Evaluating HistGradientBoostingClassifier model...\n",
      "Accuracy: 0.9560260586319218\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.87      0.91       135\n",
      "           2       0.95      0.99      0.97       345\n",
      "           3       0.98      0.96      0.97       134\n",
      "\n",
      "    accuracy                           0.96       614\n",
      "   macro avg       0.96      0.94      0.95       614\n",
      "weighted avg       0.96      0.96      0.96       614\n",
      "\n",
      "0:\tlearn: 0.8717427\ttest: 0.8648208\tbest: 0.8648208 (0)\ttotal: 318ms\tremaining: 5m 17s\n",
      "200:\tlearn: 0.9617264\ttest: 0.9511401\tbest: 0.9511401 (143)\ttotal: 47.8s\tremaining: 3m 9s\n",
      "400:\tlearn: 0.9792345\ttest: 0.9543974\tbest: 0.9543974 (327)\ttotal: 1m 34s\tremaining: 2m 21s\n",
      "600:\tlearn: 0.9902280\ttest: 0.9560261\tbest: 0.9560261 (515)\ttotal: 2m 21s\tremaining: 1m 33s\n",
      "800:\tlearn: 0.9914495\ttest: 0.9576547\tbest: 0.9576547 (762)\ttotal: 3m 7s\tremaining: 46.6s\n",
      "999:\tlearn: 0.9918567\ttest: 0.9592834\tbest: 0.9592834 (989)\ttotal: 3m 54s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9592833876\n",
      "bestIteration = 989\n",
      "\n",
      "Shrink model to first 990 iterations.\n",
      "Evaluating CatBoostClassifier model...\n",
      "Accuracy: 0.9592833876221498\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.89      0.93       135\n",
      "           2       0.95      0.99      0.97       345\n",
      "           3       0.98      0.94      0.96       134\n",
      "\n",
      "    accuracy                           0.96       614\n",
      "   macro avg       0.97      0.94      0.95       614\n",
      "weighted avg       0.96      0.96      0.96       614\n",
      "\n",
      "[0]\ttrain-mlogloss:0.74606\ttest-mlogloss:0.76335\n",
      "[1]\ttrain-mlogloss:0.54234\ttest-mlogloss:0.57206\n",
      "[2]\ttrain-mlogloss:0.40899\ttest-mlogloss:0.45197\n",
      "[3]\ttrain-mlogloss:0.31423\ttest-mlogloss:0.36475\n",
      "[4]\ttrain-mlogloss:0.24730\ttest-mlogloss:0.30633\n",
      "[5]\ttrain-mlogloss:0.19723\ttest-mlogloss:0.26181\n",
      "[6]\ttrain-mlogloss:0.15899\ttest-mlogloss:0.22854\n",
      "[7]\ttrain-mlogloss:0.13070\ttest-mlogloss:0.20454\n",
      "[8]\ttrain-mlogloss:0.10807\ttest-mlogloss:0.18597\n",
      "[9]\ttrain-mlogloss:0.09110\ttest-mlogloss:0.17451\n",
      "[10]\ttrain-mlogloss:0.07764\ttest-mlogloss:0.16602\n",
      "[11]\ttrain-mlogloss:0.06705\ttest-mlogloss:0.15979\n",
      "[12]\ttrain-mlogloss:0.05853\ttest-mlogloss:0.15523\n",
      "[13]\ttrain-mlogloss:0.05242\ttest-mlogloss:0.15172\n",
      "[14]\ttrain-mlogloss:0.04740\ttest-mlogloss:0.14879\n",
      "[15]\ttrain-mlogloss:0.04307\ttest-mlogloss:0.14809\n",
      "[16]\ttrain-mlogloss:0.04003\ttest-mlogloss:0.14597\n",
      "[17]\ttrain-mlogloss:0.03662\ttest-mlogloss:0.14551\n",
      "[18]\ttrain-mlogloss:0.03425\ttest-mlogloss:0.14538\n",
      "[19]\ttrain-mlogloss:0.03187\ttest-mlogloss:0.14691\n",
      "[20]\ttrain-mlogloss:0.03021\ttest-mlogloss:0.14687\n",
      "[21]\ttrain-mlogloss:0.02861\ttest-mlogloss:0.14577\n",
      "[22]\ttrain-mlogloss:0.02737\ttest-mlogloss:0.14551\n",
      "[23]\ttrain-mlogloss:0.02642\ttest-mlogloss:0.14582\n",
      "[24]\ttrain-mlogloss:0.02533\ttest-mlogloss:0.14552\n",
      "[25]\ttrain-mlogloss:0.02420\ttest-mlogloss:0.14631\n",
      "[26]\ttrain-mlogloss:0.02333\ttest-mlogloss:0.14672\n",
      "[27]\ttrain-mlogloss:0.02262\ttest-mlogloss:0.14577\n",
      "Evaluating XGBoost model...\n",
      "Accuracy: 0.9560260586319218\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92       135\n",
      "           1       0.94      0.99      0.97       345\n",
      "           2       0.98      0.94      0.96       134\n",
      "\n",
      "    accuracy                           0.96       614\n",
      "   macro avg       0.96      0.94      0.95       614\n",
      "weighted avg       0.96      0.96      0.96       614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff_pickle_path = \"/dave/data/df_features.pkl\"\n",
    "features_path = \"/dave/data/features_array.pkl.npy\"\n",
    "features = load_np_array_from_pickle(features_path)\n",
    "df = load_df_from_pickle(dff_pickle_path)\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(features, df[\"presentation\"])\n",
    "model1 = train_model_random_forest(X_train, y_train, X_test, y_test)\n",
    "model2 = train_model_HistGradientBoosting(X_train, y_train, X_test, y_test)\n",
    "model3 = train_catboost(X_train, y_train, X_test, y_test)\n",
    "model4 = train_xgboost(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uw/.venvs/matchnames/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:51:47] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "models = [(model1, \"/dave/data/model1\", \"forest\"), \n",
    "          (model2, \"/dave/data/model2\", \"hgb\"), \n",
    "          (model3, \"/dave/data/model3\", \"catboost\"), \n",
    "          (model4, \"/dave/data/model4\",  \"xgboost\")]\n",
    "for model, path, type in models:\n",
    "    store_model(model, path, type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process and predict a single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_document(filepath, tfidf_vectorizer):\n",
    "    # Step 1: Convert PDF document to text\n",
    "    document_text = pdf_to_text(filepath)\n",
    "    \n",
    "    # Step 2: Clean and tokenize the text\n",
    "    tokenized_text = clean_and_tokenize(document_text)\n",
    "    \n",
    "    # Create a DataFrame to hold the document text\n",
    "    df = pd.DataFrame({'tokenized_text': [tokenized_text]})\n",
    "    df['fname'] = filepath\n",
    "    \n",
    "    # Step 3: Extract specific features (if this applies directly to the text)\n",
    "    df = extract_specific_features(df)\n",
    "    \n",
    "    # Step 4: Apply keyword matching\n",
    "    for category, keyword_list in keywords.items():\n",
    "        df[category + \"_keyword\"] = df[\"tokenized_text\"].apply(\n",
    "            lambda x: check_keywords(x, keyword_list)\n",
    "        )\n",
    "    \n",
    "    # Step 5: Combine TF-IDF with keyword features and any additional features\n",
    "    combined_features, _ = combine_tfidf_keyword_additional_features(df, tfidf_vectorizer)\n",
    "    \n",
    "    return combined_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = '/dave/presentations/WBA-3Q-2023-Presentation.pdf'\n",
    "os.path.exists(fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 1/1 [00:00<00:00, 15.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Prediction: [1]\n",
      "Time taken for single prediction: 1.414553165435791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# For a single prediction from each model\n",
    "start = time.time()\n",
    "doc_feature_array = prepare_single_document(fn, tdif_vectorizer)\n",
    "doc_feature_array.shape\n",
    "single_prediction_model1 = model1.predict(doc_feature_array)\n",
    "print(\"Random Forest Prediction:\", single_prediction_model1)\n",
    "print(\"Time taken for single prediction:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistGradientBoosting Prediction: [1]\n",
      "Time taken: 0.0031778812408447266\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "single_prediction_model2 = model2.predict(doc_feature_array)\n",
    "print(\"HistGradientBoosting Prediction:\", single_prediction_model2)\n",
    "print(\"Time taken:\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Prediction: [[1]]\n",
      "Time taken: 0.012895822525024414\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "single_prediction_model3 = model3.predict(doc_feature_array)\n",
    "print(\"CatBoost Prediction:\", single_prediction_model3)\n",
    "print(\"Time taken:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Prediction: [0.]\n",
      "Time taken: 0.03315234184265137\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dtest = xgb.DMatrix(doc_feature_array)\n",
    "\n",
    "single_prediction_model4 = model4.predict(dtest)\n",
    "print(\"XGBoost Prediction:\", single_prediction_model4)\n",
    "\n",
    "print(\"Time taken:\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on processing links in memory here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_download(initial_files, download_dir, url):\n",
    "    # Wait for the download to start and finish\n",
    "    # Adjust the time as needed based on your expected download time\n",
    "    # time.sleep(2)  # Initial sleep to wait for the download to start\n",
    "\n",
    "    # Now wait for a new file to appear in the directory\n",
    "    new_file = None\n",
    "    timeout = 10  # Max time to wait for a download to finish\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        current_files = set(os.listdir(download_dir))\n",
    "        new_files = current_files - initial_files\n",
    "        if new_files:\n",
    "            new_file = new_files.pop()\n",
    "            break\n",
    "        elif time.time() - start_time > timeout:\n",
    "            print(f\"Timeout waiting for download to complete for {link}\")\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(1)  # Check every second for a new file\n",
    "\n",
    "    if new_file:\n",
    "        new_file =  os.path.join(download_dir, new_file)\n",
    "        doc_feature_array = prepare_single_document(new_file, tdif_vectorizer)\n",
    "                    \n",
    "        single_prediction_model2 = model2.predict(doc_feature_array)\n",
    "        # remove the file from the download directory\n",
    "        os.remove(os.path.join(download_dir, new_file))\n",
    "        return  single_prediction_model2\n",
    "    else:\n",
    "        logging.info(f\"Failed to download {url}\")\n",
    "        return None\n",
    "\n",
    "def download_file(url, download_path):\n",
    "    # Set up Firefox profile to handle downloads automatically\n",
    "    gecko_driver_path = \"/snap/bin/geckodriver\"\n",
    "\n",
    "    # Set up Firefox options\n",
    "    firefox_options = Options()\n",
    "    firefox_options.add_argument(\"--headless\")\n",
    "    firefox_options.set_preference(\"general.useragent.override\", getRandomAgent())\n",
    "\n",
    "    # Create a Firefox Profile\n",
    "    # firefox_profile = webdriver.FirefoxProfile()\n",
    "    firefox_options.set_preference(\"browser.download.folderList\", 2)\n",
    "    firefox_options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "    firefox_options.set_preference(\"browser.download.dir\", download_path)\n",
    "    firefox_options.set_preference(\"browser.download.useDownloadDir\", True)\n",
    "    firefox_options.set_preference(\n",
    "        \"browser.helperApps.neverAsk.saveToDisk\", \"application/pdf\"\n",
    "    )\n",
    "    firefox_options.enable_downloads = True\n",
    "\n",
    "    firefox_options.set_preference(\n",
    "        \"pdfjs.disabled\", True\n",
    "    )  # Disable Firefox's built-in PDF viewer\n",
    "    \n",
    "    # Initialize the driver with Service\n",
    "    service = Service(executable_path=gecko_driver_path)\n",
    "    driver = webdriver.Firefox(service=service, options=firefox_options)\n",
    "    driver.set_page_load_timeout(5)\n",
    "    #\n",
    "    # Navigate to URL and initiate download\n",
    "    initial_files = set(os.listdir(download_path ))\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        handle_download(initial_files,  download_path, url)\n",
    "        print(f\"Downloading {url} to {download_path}???\")\n",
    "    except TimeoutException:\n",
    "        prediction = handle_download(initial_files, download_path, url)\n",
    "\n",
    "        driver.quit()\n",
    "        return prediction\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {str(e)}\")\n",
    "        driver.quit()\n",
    "        return \"\"\n",
    "\n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "# Open dataset.csv as dataframe\n",
    "def open_dataset(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matchnames",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
