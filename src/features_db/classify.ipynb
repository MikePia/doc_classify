{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import fitz  # PyMuPDF\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='document_processing_errors.log', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(path):\n",
    "    try:\n",
    "        doc = fitz.open(path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            try:\n",
    "                text += page.get_text()\n",
    "            except Exception as page_error:\n",
    "                print(f\"Error extracting text from page in {path}: {page_error}\")\n",
    "                continue\n",
    "                # Optionally, continue to the next page or log the error\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Error processing file {path}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use one or the othere here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Or a larger model as needed\n",
    "\n",
    "def clean_and_tokenize(text, chunk_size=1000000):\n",
    "    \"\"\"\n",
    "    Tokenizes the text using SpaCy, handling long texts by processing in chunks.\n",
    "    \n",
    "    :param text: The text to be tokenized.\n",
    "    :param chunk_size: Maximum chunk size in characters.\n",
    "    :return: A string of the lemmatized tokens.\n",
    "    \"\"\"\n",
    "    # Check if the text length exceeds the chunk size\n",
    "    if len(text) > chunk_size:\n",
    "        # Initialize an empty list to store tokens from all chunks\n",
    "        tokens_all_chunks = []\n",
    "        \n",
    "        # Process the text in chunks\n",
    "        for start in range(0, len(text), chunk_size):\n",
    "            end = start + chunk_size\n",
    "            # Extract a chunk of text\n",
    "            chunk = text[start:end]\n",
    "            # Process the chunk\n",
    "            doc = nlp(chunk)\n",
    "            # Extract tokens, lemmatize, and filter as before\n",
    "            tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "            tokens_all_chunks.extend(tokens)\n",
    "        \n",
    "        # Combine tokens from all chunks and return\n",
    "        return \" \".join(tokens_all_chunks)\n",
    "    else:\n",
    "        # If text does not exceed the chunk size, process as before\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "# Example of how to apply this function to your DataFrame\n",
    "# df['tokenized_text'] = df['cleaned_text'].apply(clean_and_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize_chunk(chunk):\n",
    "    \"\"\"\n",
    "    Tokenizes a single chunk of text.\n",
    "    \"\"\"\n",
    "    doc = nlp(chunk)\n",
    "    return [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "def batch_tokenize_texts(texts, batch_size=1000, chunk_size=1000000):\n",
    "    \"\"\"\n",
    "    Tokenize a list of texts in batches, handling long texts by processing in chunks.\n",
    "    \n",
    "    :param texts: The list of texts to be tokenized.\n",
    "    :param batch_size: Number of texts to process in a single batch.\n",
    "    :param chunk_size: Maximum chunk size in characters for each text.\n",
    "    :return: A list of lists, where each sublist contains the tokens of a text.\n",
    "    \"\"\"\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        # If the text is longer than chunk_size, split it into chunks\n",
    "        if len(text) > chunk_size:\n",
    "            tokens_all_chunks = []\n",
    "            for start in range(0, len(text), chunk_size):\n",
    "                end = start + chunk_size\n",
    "                chunk = text[start:end]\n",
    "                # Tokenize the chunk and extend the list of tokens\n",
    "                tokens_all_chunks.extend(clean_and_tokenize_chunk(chunk))\n",
    "            processed_texts.append(tokens_all_chunks)\n",
    "        else:\n",
    "            # For texts that don't exceed the chunk size, process as usual\n",
    "            tokens = clean_and_tokenize_chunk(text)\n",
    "            processed_texts.append(tokens)\n",
    "    \n",
    "    return processed_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Extraction\n",
    "- **Combine keyword-matching and TF-IDF**\n",
    "- **TF-IDF Vectorization:** Use Scikit-learn's TfidfVectorizer to convert the cleaned text documents into a matrix of TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def check_aspect_ratio_and_mix_feature(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Error opening PDF {pdf_path}: {e}\")\n",
    "        return [], 0, False  # Returning False as the third value for no significant change\n",
    "    \n",
    "    aspect_ratios = []\n",
    "    \n",
    "    page_count = len(doc)\n",
    "    \n",
    "    for page in doc:\n",
    "        rect = page.rect\n",
    "        aspect_ratio = rect.width / rect.height\n",
    "        aspect_ratios.append(aspect_ratio)\n",
    "    \n",
    "    # Detect significant changes in aspect ratio\n",
    "    aspect_threshold = detect_significant_change(aspect_ratios)\n",
    "    features = detect_persistent_changes(aspect_ratios, aspect_threshold)\n",
    "    num_changes,changes = detect_significant_change_enhanced(aspect_ratios, aspect_threshold)\n",
    "    stats = extract_aspect_ratio_features(aspect_ratios)\n",
    "    kmeans_labels = cluster_aspect_ratios(aspect_ratios)\n",
    "    # outliers = detect_outliers_z_score(aspect_ratios, kmeans_labels)\n",
    "    \n",
    "    return (aspect_ratios, page_count, aspect_threshold, features, num_changes, changes, stats, kmeans_labels)\n",
    "\n",
    "def detect_significant_change(aspect_ratios, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Detects significant changes in aspect ratio.\n",
    "    :param aspect_ratios: List of aspect ratios for the document's pages.\n",
    "    :param threshold: The threshold for detecting a significant change.\n",
    "    :return: True if a significant change is detected, otherwise False.\n",
    "    \"\"\"\n",
    "    for i in range(1, len(aspect_ratios)):\n",
    "        new_t =  abs(aspect_ratios[i] - aspect_ratios[i-1]) / aspect_ratios[i-1]\n",
    "        threshold = max(threshold, new_t)\n",
    "\n",
    "    return threshold\n",
    "\n",
    "\n",
    "def detect_significant_change_enhanced(aspect_ratios, threshold=0.1):\n",
    "    changes = np.diff(aspect_ratios) / aspect_ratios[:-1]\n",
    "    mean_change = np.mean(changes)\n",
    "    std_dev_change = np.std(changes)\n",
    "    \n",
    "    significant_changes = changes[(changes > mean_change + std_dev_change * threshold) | \n",
    "                                  (changes < mean_change - std_dev_change * threshold)]\n",
    "    return len(significant_changes) > 0, significant_changes\n",
    "\n",
    "\n",
    "\n",
    "def detect_persistent_changes(aspect_ratios, change_threshold=0.1, persistence_threshold=3):\n",
    "    \"\"\"\n",
    "    Detects persistent changes in aspect ratios.\n",
    "    \n",
    "    :param aspect_ratios: List of aspect ratios for each page in the document.\n",
    "    :param change_threshold: The minimum change in aspect ratio to consider.\n",
    "    :param persistence_threshold: The minimum number of consecutive pages over which a change must persist to be considered.\n",
    "    :return: A feature set capturing aspects of persistent changes.\n",
    "    \"\"\"\n",
    "    changes = [abs(aspect_ratios[i] - aspect_ratios[i-1]) / aspect_ratios[i-1] for i in range(1, len(aspect_ratios))]\n",
    "    persistent_changes = 0\n",
    "    current_persistence = 0\n",
    "    \n",
    "    for change in changes:\n",
    "        if change > change_threshold:\n",
    "            current_persistence += 1\n",
    "        else:\n",
    "            if current_persistence >= persistence_threshold:\n",
    "                persistent_changes += 1\n",
    "            current_persistence = 0\n",
    "    \n",
    "    # Catch any sequence that goes until the end of the document\n",
    "    if current_persistence >= persistence_threshold:\n",
    "        persistent_changes += 1\n",
    "    \n",
    "    # Example features: count of persistent changes, presence of any persistent change\n",
    "    features = {\n",
    "        'persistent_change_count': persistent_changes,\n",
    "        'has_persistent_change': int(persistent_changes > 0)\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def extract_aspect_ratio_features(aspect_ratios):\n",
    "    \"\"\"\n",
    "    Extracts statistical features from a list of aspect ratios.\n",
    "    \n",
    "    :param aspect_ratios: List of aspect ratios for the document's pages.\n",
    "    :return: Dictionary of statistical features.\n",
    "    \"\"\"\n",
    "    if not aspect_ratios:  # Check if the list is empty\n",
    "        return {\n",
    "            'mean': 0,\n",
    "            'std': 0,\n",
    "            'min': 0,\n",
    "            'max': 0\n",
    "        }\n",
    "    \n",
    "    aspect_ratios_array = np.array(aspect_ratios)\n",
    "    return {\n",
    "        'mean': np.mean(aspect_ratios_array),\n",
    "        'std': np.std(aspect_ratios_array),\n",
    "        'min': np.min(aspect_ratios_array),\n",
    "        'max': np.max(aspect_ratios_array)\n",
    "    }\n",
    "\n",
    "\n",
    "def cluster_aspect_ratios(aspect_ratios, n_clusters=3):\n",
    "    \"\"\"\n",
    "    Clusters aspect ratios into n clusters.\n",
    "    \n",
    "    :param aspect_ratios: List of aspect ratios for the document's pages.\n",
    "    :param n_clusters: Number of clusters to form.\n",
    "    :return: Labels for each page indicating cluster membership.\n",
    "    \"\"\"\n",
    "    aspect_ratios = np.array(aspect_ratios).reshape(-1, 1)  # Reshape for clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(aspect_ratios)\n",
    "    return kmeans.labels_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def detect_outliers_z_score(aspect_ratios, threshold=2):\n",
    "    aspect_ratios = np.array(aspect_ratios).flatten()  # Ensures aspect_ratios is 1D\n",
    "    mean_ar = np.mean(aspect_ratios)\n",
    "    std_ar = np.std(aspect_ratios)\n",
    "    outliers = [i for i, ar in enumerate(aspect_ratios) if abs((ar - mean_ar) / std_ar) > threshold]\n",
    "    \n",
    "    return outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for keyword presence\n",
    "def check_keywords(text, keyword_list):\n",
    "    text = text.lower()\n",
    "    return int(any(keyword in text for keyword in keyword_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# alternate code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def check_keywords_alternate(text, keyword_list):\n",
    "    text = text.lower()\n",
    "    # Create a pattern that matches whole words only, for all keywords\n",
    "    pattern = r'\\b(' + '|'.join([re.escape(keyword) for keyword in keyword_list]) + r')\\b'\n",
    "    return int(bool(re.search(pattern, text)))\n",
    "\n",
    "def safety_not_run_thing():\n",
    "    # Convert keywords to lowercase for case-insensitive matching\n",
    "    keywords = {category: [keyword.lower() for keyword in keyword_list] for category, keyword_list in keywords.items()}\n",
    "\n",
    "    # Assuming 'tokenized_text' contains space-separated tokens, it should work well with the modified check_keywords function.\n",
    "    # Just ensure 'tokenized_text' is a string; if it's a list of tokens, you might need to join them first:\n",
    "    # df['tokenized_text_str'] = df['tokenized_text'].apply(' '.join)\n",
    "\n",
    "    for category, keyword_list in keywords.items():\n",
    "        df[category + '_keyword'] = df['tokenized_text'].apply(check_keywords, args=(keyword_list,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tfidf_keyword(df):\n",
    "    # Step 2: TF-IDF Calculation\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)  # Adjust number of features as needed\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['tokenized_text'])\n",
    "\n",
    "    # Step 3. Combine keyword and tfidf features into a single matrix    \n",
    "    # Convert binary keyword matches to a matrix\n",
    "    keyword_features = df[[col for col in df.columns if '_keyword' in col]].to_numpy()\n",
    "    # Combine TF-IDF features with keyword binary indicators\n",
    "    combined_features = np.hstack((tfidf_matrix.toarray(), keyword_features))\n",
    "\n",
    "    # Now `combined_features` is ready for model training, and should be aligned with your labels.\n",
    "    return combined_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training the Classification Model\n",
    "\n",
    "### Next Steps (not-implemented)\n",
    "\n",
    "    Train Your Model: Use the combined_features matrix along with your labels to train and evaluate your classification model.\n",
    "    Evaluation and Refinement: Assess the model's performance and adjust your keyword lists, TF-IDF parameters, or model choice as needed.\n",
    "\n",
    "This approach leverages both the specificity of keyword matching and the nuanced importance scoring of TF-IDF, providing a rich set of features for document classification.\n",
    "\n",
    "- **Splitting Data:** Use your 1000 classified documents as training data. Ensure you have a balanced dataset for the three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y):\n",
    "    # Split the data - 70% for training, 30% for testing; adjust ratios as you see fit\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Model Selection and Training:** Given the textual nature of your task, models like CNN or LSTM could perform well. TensorFlow/Keras will be used here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An initial simple Binary logistic  regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X_train, y_train):\n",
    "\n",
    "    # Initialize the Logistic Regression model\n",
    "    model = LogisticRegression(max_iter=1000)  # Increasing max_iter for convergence\n",
    "\n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Return the trained model\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not using the NN below yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X_train, y_train, X_test, y_test):\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=5000, output_dim=64))  # Adjust according to the TF-IDF feature size\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3, activation='softmax'))  # Three categories\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Classification of New Documents --- not ready:\n",
    "- **Predicting Categories:** Use the trained model to predict categories for new documents after preprocessing and vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(text):\n",
    "    clean_text = clean_and_tokenize(text)\n",
    "    vectorized_text = vectorizer.transform([clean_text])\n",
    "    prediction = model.predict(vectorized_text)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluation and Iteration not ready\n",
    "\n",
    "- **Evaluation:** Use metrics like accuracy, precision, recall, and F1 score to evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Iteration:** Based on evaluation results, iterate over your model by tuning hyperparameters, trying different models (e.g., BERT for text classification), or using more advanced text vectorization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Scalability and Optimization\n",
    "\n",
    "- Consider parallel processing or distributed computing for preprocessing steps if you face performance bottlenecks.\n",
    "- Explore incremental learning or online learning models if retraining on new data frequently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to load df from pickle file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "dataset_path = '../../data/dataset.csv'\n",
    "DATA_PATH = '/dave/presentations/'\n",
    "\n",
    "dfpickle_path = \"/dave/data/df.pkl\"\n",
    "force = False\n",
    "if not os.path.exists(dfpickle_path) or force:\n",
    "    print(\"Warning, is dave mounted?\")\n",
    "else:\n",
    "    print('Going to load df from pickle file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.exists(dfpickle_path) and not force:\n",
    "    df = pd.read_pickle(dfpickle_path)\n",
    "else:\n",
    "    df = pd.read_csv(dataset_path, header=0)\n",
    "    df['fname'] = DATA_PATH + df['fname']\n",
    "\n",
    "    tqdm.pandas(desc=\"Processing documents\")\n",
    "    df['cleaned_text'] = df['fname'].progress_apply(pdf_to_text)\n",
    "\n",
    "    df['tokenized_text'] = df['cleaned_text'].progress_apply(clean_and_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m testit \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/dave/presentations/p23-0016_exhibit1.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m (\n\u001b[1;32m      3\u001b[0m     aspect_ratios,\n\u001b[1;32m      4\u001b[0m     page_count,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     kmeans_labels,\n\u001b[1;32m     11\u001b[0m ) \u001b[38;5;241m=\u001b[39m check_aspect_ratio_and_mix_feature(testit)\n\u001b[1;32m     12\u001b[0m detect_outliers_z_score(aspect_ratios)\n",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m testit \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/dave/presentations/p23-0016_exhibit1.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m (\n\u001b[1;32m      3\u001b[0m     aspect_ratios,\n\u001b[1;32m      4\u001b[0m     page_count,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     kmeans_labels,\n\u001b[1;32m     11\u001b[0m ) \u001b[38;5;241m=\u001b[39m check_aspect_ratio_and_mix_feature(testit)\n\u001b[1;32m     12\u001b[0m detect_outliers_z_score(aspect_ratios)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/uw/.venvs/matchnames/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m/uw/.venvs/matchnames/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "testit = \"/dave/presentations/p23-0016_exhibit1.pdf\"\n",
    "(\n",
    "    aspect_ratios,\n",
    "    page_count,\n",
    "    aspect_threshold,\n",
    "    features,\n",
    "    num_changes,\n",
    "    changes,\n",
    "    stats,\n",
    "    kmeans_labels,\n",
    ") = check_aspect_ratio_and_mix_feature(testit)\n",
    "\n",
    "print(f\"aspect_ratios:  {aspect_ratios}\")\n",
    "print(f\"page_count:  {page_count}\")\n",
    "print(f\"aspect_threshold:  {aspect_threshold}\")\n",
    "print(f\"features:  {features}\")\n",
    "print(f\"num_changes:  {num_changes}\")\n",
    "print(f\"changes:  {changes}\")\n",
    "print(f\"mean: {stats['mean']}\")\n",
    "print(f\"std: {stats['std']}\")\n",
    "print(f\"min: {stats['min']}\")\n",
    "print(f\"max: {stats['max']}\")\n",
    "print(f\"kmeans_labels:  {kmeans_labels}\")\n",
    "# print(f\"outliers:  {outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = detect_outliers_z_score(aspect_ratios)\n",
    "print(f\"outliers: {outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {\n",
    "    \"financial_terms\": ['financial', 'investment', 'share price', 'financial metrics', 'investment strategy'],\n",
    "    \"legal_statements\": ['confidentiality statement', 'legal disclaimer', 'disclosure statement', 'proprietary information', 'intellectual property'],\n",
    "    \"company_info\": ['company overview', 'company analysis', 'business model', 'company performance'],\n",
    "    \"presentation_content\": ['visual aids', 'data charts', 'case studies', 'comparative analysis'],\n",
    "    \"company_targets\": ['sales targets', 'company targets', 'performance targets'],\n",
    "    \"financial_discussions\": ['financial figures', 'financial projections', 'financial results', 'financial language'],\n",
    "    \"regulatory_references\": ['SEC filings', 'regulatory filings', 'external entities', 'lawsuits'],\n",
    "    \"detail_descriptions\": ['loan details', 'product details', 'research and development', 'financial details'],\n",
    "    \"company_specific\": ['company specific', 'industry specific', 'company-specific analysis', 'specific company focus']\n",
    "    # \"Other Clusters\" category is omitted since it's broad and without specific keywords\n",
    "}\n",
    "\n",
    "\n",
    "# Apply keyword matching\n",
    "for category, keyword_list in keywords.items():\n",
    "    df[category + '_keyword'] = df['tokenized_text'].apply(check_keywords, args=(keyword_list,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = combine_tfidf_keyword(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(features, df['presentation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='binary'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='binary'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average='binary'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame `df_test` corresponding to your test dataset\n",
    "# And it includes a column 'doc_id' or similar that uniquely identifies each document\n",
    "# If you don't have such a DataFrame, you can create it from `X_test` and `y_test`\n",
    "\n",
    "\n",
    "type(X_test), type(y_test)\n",
    "# df_test = pd.DataFrame({'doc_id': X_test.index, 'text': X_test, 'label': y_test})\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First, ensure `X_test` retains its index after splitting so you can merge based on index\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'True Label': y_test,\n",
    "    'Predicted Label': y_pred\n",
    "})\n",
    "\n",
    "# If `X_test` and `y_test` don't automatically align, you might need to reset the index\n",
    "# misclassified_df = misclassified_df.reset_index()\n",
    "\n",
    "# Add a column to indicate whether each prediction is correct\n",
    "misclassified_df['Correctly Classified'] = misclassified_df['True Label'] == misclassified_df['Predicted Label']\n",
    "\n",
    "# Filter the DataFrame to only include misclassified documents\n",
    "misclassified_docs = misclassified_df[~misclassified_df['Correctly Classified']]\n",
    "\n",
    "# Optionally, join with the original DataFrame (df) to include text or other identifying information\n",
    "# This step requires that `df` and `misclassified_docs` can be aligned by index or a unique identifier\n",
    "# Example:\n",
    "# misclassified_docs = misclassified_docs.join(df[['doc_id', 'text']], how='left')\n",
    "\n",
    "misclassified_docs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matchnames",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
