{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import fitz  # PyMuPDF\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='document_processing_errors.log', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(path):\n",
    "    try:\n",
    "        doc = fitz.open(path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            try:\n",
    "                text += page.get_text()\n",
    "            except Exception as page_error:\n",
    "                print(f\"Error extracting text from page in {path}: {page_error}\")\n",
    "                continue\n",
    "                # Optionally, continue to the next page or log the error\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Error processing file {path}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider using this to only run stuff once through the get_text() procedure:\n",
    "nOTE THAT THIS  WOULD REPLACE TWO METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import logging\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Error opening PDF {pdf_path}: {e}\")\n",
    "        return None  # Indicate failure to open PDF\n",
    "\n",
    "    text = \"\"\n",
    "    aspect_ratios = []\n",
    "    for page in doc:\n",
    "        # Extract text\n",
    "        try:\n",
    "            text += page.get_text()\n",
    "        except Exception as page_error:\n",
    "            logging.info(f\"Error extracting text from page in {pdf_path}: {page_error}\")\n",
    "            # Optionally, continue to the next page or log the error\n",
    "\n",
    "        # Calculate aspect ratio\n",
    "        rect = page.rect\n",
    "        aspect_ratio = rect.width / rect.height\n",
    "        aspect_ratios.append(aspect_ratio)\n",
    "    \n",
    "    # Process aspect ratios for features\n",
    "    aspect_threshold = detect_significant_change(aspect_ratios)\n",
    "    features = detect_persistent_changes(aspect_ratios, aspect_threshold)\n",
    "    num_changes, changes = detect_significant_change_enhanced(aspect_ratios, aspect_threshold)\n",
    "    stats = extract_aspect_ratio_features(aspect_ratios)\n",
    "    categories = categorize_aspect_ratios(aspect_ratios)\n",
    "    \n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"aspect_ratios\": aspect_ratios,\n",
    "        \"page_count\": len(doc),\n",
    "        \"aspect_threshold\": aspect_threshold,\n",
    "        \"features\": features,\n",
    "        \"num_changes\": num_changes,\n",
    "        \"changes\": changes,\n",
    "        \"stats\": stats,\n",
    "        \"categories\": categories,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use one or the othere here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")  # Or a larger model as needed\n",
    "\n",
    "def clean_and_tokenize(text, chunk_size=1000000):\n",
    "    \"\"\"\n",
    "    Tokenizes the text using SpaCy, handling long texts by processing in chunks.\n",
    "    \n",
    "    :param text: The text to be tokenized.\n",
    "    :param chunk_size: Maximum chunk size in characters.\n",
    "    :return: A string of the lemmatized tokens.\n",
    "    \"\"\"\n",
    "    # Check if the text length exceeds the chunk size\n",
    "    if len(text) > chunk_size:\n",
    "        # Initialize an empty list to store tokens from all chunks\n",
    "        tokens_all_chunks = []\n",
    "        \n",
    "        # Process the text in chunks\n",
    "        for start in range(0, len(text), chunk_size):\n",
    "            end = start + chunk_size\n",
    "            # Extract a chunk of text\n",
    "            chunk = text[start:end]\n",
    "            # Process the chunk\n",
    "            doc = nlp(chunk)\n",
    "            # Extract tokens, lemmatize, and filter as before\n",
    "            tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "            tokens_all_chunks.extend(tokens)\n",
    "        \n",
    "        # Combine tokens from all chunks and return\n",
    "        return \" \".join(tokens_all_chunks)\n",
    "    else:\n",
    "        # If text does not exceed the chunk size, process as before\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "# Example of how to apply this function to your DataFrame\n",
    "# df['tokenized_text'] = df['cleaned_text'].apply(clean_and_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize_chunk(chunk):\n",
    "    \"\"\"\n",
    "    Tokenizes a single chunk of text.\n",
    "    \"\"\"\n",
    "    doc = nlp(chunk)\n",
    "    return [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "def batch_tokenize_texts(texts, batch_size=1000, chunk_size=1000000):\n",
    "    \"\"\"\n",
    "    Tokenize a list of texts in batches, handling long texts by processing in chunks.\n",
    "    \n",
    "    :param texts: The list of texts to be tokenized.\n",
    "    :param batch_size: Number of texts to process in a single batch.\n",
    "    :param chunk_size: Maximum chunk size in characters for each text.\n",
    "    :return: A list of lists, where each sublist contains the tokens of a text.\n",
    "    \"\"\"\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        # If the text is longer than chunk_size, split it into chunks\n",
    "        if len(text) > chunk_size:\n",
    "            tokens_all_chunks = []\n",
    "            for start in range(0, len(text), chunk_size):\n",
    "                end = start + chunk_size\n",
    "                chunk = text[start:end]\n",
    "                # Tokenize the chunk and extend the list of tokens\n",
    "                tokens_all_chunks.extend(clean_and_tokenize_chunk(chunk))\n",
    "            processed_texts.append(tokens_all_chunks)\n",
    "        else:\n",
    "            # For texts that don't exceed the chunk size, process as usual\n",
    "            tokens = clean_and_tokenize_chunk(text)\n",
    "            processed_texts.append(tokens)\n",
    "    \n",
    "    return processed_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Extraction\n",
    "- **Combine keyword-matching and TF-IDF**\n",
    "- **TF-IDF Vectorization:** Use Scikit-learn's TfidfVectorizer to convert the cleaned text documents into a matrix of TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_aspect_ratio_and_mix_feature(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Error opening PDF {pdf_path}: {e}\")\n",
    "        raise ValueError(\"Error opening PDF\")\n",
    "        return [], 0, False  # Returning False as the third value for no significant change\n",
    "    \n",
    "    aspect_ratios = []\n",
    "    \n",
    "    page_count = len(doc)\n",
    "    \n",
    "    for page in doc:\n",
    "        rect = page.rect\n",
    "        aspect_ratio = rect.width / rect.height\n",
    "        aspect_ratios.append(aspect_ratio)\n",
    "    \n",
    "    # Detect significant changes in aspect ratio\n",
    "    aspect_threshold = detect_significant_change(aspect_ratios)\n",
    "    features = detect_persistent_changes(aspect_ratios, aspect_threshold)\n",
    "    num_changes,changes = detect_significant_change_enhanced(aspect_ratios, aspect_threshold)\n",
    "    stats = extract_aspect_ratio_features(aspect_ratios)\n",
    "    categories = categorize_aspect_ratios(aspect_ratios)\n",
    "    \n",
    "    return (aspect_ratios, page_count, aspect_threshold, features, num_changes, changes, stats, categories)\n",
    "\n",
    "def detect_significant_change(aspect_ratios, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Detects significant changes in aspect ratio.\n",
    "    :param aspect_ratios: List of aspect ratios for the document's pages.\n",
    "    :param threshold: The threshold for detecting a significant change.\n",
    "    :return: True if a significant change is detected, otherwise False.\n",
    "    \"\"\"\n",
    "    for i in range(1, len(aspect_ratios)):\n",
    "        new_t =  abs(aspect_ratios[i] - aspect_ratios[i-1]) / aspect_ratios[i-1]\n",
    "        threshold = max(threshold, new_t)\n",
    "\n",
    "    return threshold\n",
    "\n",
    "\n",
    "def detect_significant_change_enhanced(aspect_ratios, threshold=0.1):\n",
    "    changes = np.diff(aspect_ratios) / aspect_ratios[:-1]\n",
    "    mean_change = np.mean(changes)\n",
    "    std_dev_change = np.std(changes)\n",
    "    \n",
    "    significant_changes = changes[(changes > mean_change + std_dev_change * threshold) | \n",
    "                                  (changes < mean_change - std_dev_change * threshold)]\n",
    "    return len(significant_changes) > 0, significant_changes\n",
    "\n",
    "\n",
    "\n",
    "def detect_persistent_changes(aspect_ratios, change_threshold=0.1, persistence_threshold=3):\n",
    "    \"\"\"\n",
    "    Detects persistent changes in aspect ratios.\n",
    "    \n",
    "    :param aspect_ratios: List of aspect ratios for each page in the document.\n",
    "    :param change_threshold: The minimum change in aspect ratio to consider.\n",
    "    :param persistence_threshold: The minimum number of consecutive pages over which a change must persist to be considered.\n",
    "    :return: A feature set capturing aspects of persistent changes.\n",
    "    \"\"\"\n",
    "    changes = [abs(aspect_ratios[i] - aspect_ratios[i-1]) / aspect_ratios[i-1] for i in range(1, len(aspect_ratios))]\n",
    "    persistent_changes = 0\n",
    "    current_persistence = 0\n",
    "    \n",
    "    for change in changes:\n",
    "        if change > change_threshold:\n",
    "            current_persistence += 1\n",
    "        else:\n",
    "            if current_persistence >= persistence_threshold:\n",
    "                persistent_changes += 1\n",
    "            current_persistence = 0\n",
    "    \n",
    "    # Catch any sequence that goes until the end of the document\n",
    "    if current_persistence >= persistence_threshold:\n",
    "        persistent_changes += 1\n",
    "    \n",
    "    # Example features: count of persistent changes, presence of any persistent change\n",
    "    features = {\n",
    "        'persistent_change_count': persistent_changes,\n",
    "        'has_persistent_change': int(persistent_changes > 0)\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def extract_aspect_ratio_features(aspect_ratios):\n",
    "    \"\"\"\n",
    "    Extracts statistical features from a list of aspect ratios.\n",
    "    \n",
    "    :param aspect_ratios: List of aspect ratios for the document's pages.\n",
    "    :return: Dictionary of statistical features.\n",
    "    \"\"\"\n",
    "    if not aspect_ratios:  # Check if the list is empty\n",
    "        return {\n",
    "            'mean': 0,\n",
    "            'std': 0,\n",
    "            'min': 0,\n",
    "            'max': 0\n",
    "        }\n",
    "    \n",
    "    aspect_ratios_array = np.array(aspect_ratios)\n",
    "    return {\n",
    "        'mean': np.mean(aspect_ratios_array),\n",
    "        'std': np.std(aspect_ratios_array),\n",
    "        'min': np.min(aspect_ratios_array),\n",
    "        'max': np.max(aspect_ratios_array)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def categorize_aspect_ratios(aspect_ratios):\n",
    "    \"\"\"\n",
    "    Categorizes aspect ratios as portrait, landscape, or square.\n",
    "    \n",
    "    :param aspect_ratios: List of aspect ratios for the document's pages.\n",
    "    :return: List of categories corresponding to each aspect ratio.\n",
    "    \"\"\"\n",
    "    categories = []\n",
    "    for ar in aspect_ratios:\n",
    "        if ar < 0.95:\n",
    "            categories.append('portrait')\n",
    "        elif ar > 1.05:\n",
    "            categories.append('landscape')\n",
    "        else:\n",
    "            categories.append('square')\n",
    "    return categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cluster_aspect_ratios(aspect_ratios, n_clusters=3):\n",
    "    \"\"\"\n",
    "    Clusters aspect ratios into n clusters.\n",
    "    \n",
    "    :param aspect_ratios: List of aspect ratios for the document's pages.\n",
    "    :param n_clusters: Number of clusters to form.\n",
    "    :return: Labels for each page indicating cluster membership.\n",
    "    \"\"\"\n",
    "    aspect_ratios = np.array(aspect_ratios).reshape(-1, 1)  # Reshape for clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(aspect_ratios)\n",
    "    return kmeans.labels_\n",
    "\n",
    "\n",
    "\n",
    "def calculate_features(cluster_assignments):\n",
    "    features = []\n",
    "    for document in cluster_assignments:\n",
    "        # Feature 1: Cluster Transition Indicator (CTI)\n",
    "        transitions = sum([1 for i in range(1, len(document)) if document[i] != document[i-1]])\n",
    "        \n",
    "        # Feature 2: Cluster Distribution Spread (CDS)\n",
    "        cluster_counts = np.bincount(document, minlength=3)  # Assuming 3 clusters\n",
    "        proportions = cluster_counts / np.sum(cluster_counts)\n",
    "        spread = entropy(proportions)  # Using entropy as a measure of spread\n",
    "        \n",
    "        features.append((transitions, spread))\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_z_score(aspect_ratios, threshold=2):\n",
    "    aspect_ratios = np.array(aspect_ratios).flatten()  # Ensures aspect_ratios is 1D\n",
    "    mean_ar = np.mean(aspect_ratios)\n",
    "    std_ar = np.std(aspect_ratios)\n",
    "    outliers = [i for i, ar in enumerate(aspect_ratios) if abs((ar - mean_ar) / std_ar) > threshold]\n",
    "    \n",
    "    return outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "def calculate_text_density(pdf_path):\n",
    "    \"\"\"\n",
    "    Calculates text density (words per page) for a PDF document.\n",
    "    \n",
    "    :param pdf_path: Path to the PDF document.\n",
    "    :return: List of text densities for each page.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_densities = []\n",
    "    for page in doc:\n",
    "        text = page.get_text(\"text\")\n",
    "        word_count = len(text.split())\n",
    "        area = page.rect.width * page.rect.height\n",
    "        text_density = word_count / area if area else 0\n",
    "        text_densities.append(text_density)\n",
    "    return text_densities\n",
    "\n",
    "def correlate_aspect_ratio_text_density(aspect_ratios, text_densities):\n",
    "    \"\"\"\n",
    "    Correlates aspect ratios with text densities.\n",
    "    \n",
    "    :param aspect_ratios: List of aspect ratios for the document's pages.\n",
    "    :param text_densities: List of text densities for each page.\n",
    "    :return: Correlation coefficient between aspect ratios and text densities.\n",
    "    \"\"\"\n",
    "    if len(aspect_ratios) != len(text_densities):\n",
    "        raise ValueError(\"Aspect ratios and text densities lists must be of the same length.\")\n",
    "    \n",
    "    # Check standard deviation to decide on correlation calculation\n",
    "    std_aspect_ratios = np.nanstd(aspect_ratios)\n",
    "    std_text_densities = np.nanstd(text_densities)\n",
    "    \n",
    "    if std_aspect_ratios > 0 and std_text_densities > 0:\n",
    "        correlation = np.corrcoef(aspect_ratios, text_densities)[0, 1]\n",
    "    else:\n",
    "        correlation = np.nan \n",
    "    \n",
    "    return correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for keyword presence\n",
    "def check_keywords(text, keyword_list):\n",
    "    text = text.lower()\n",
    "    return int(any(keyword in text for keyword in keyword_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# alternate code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def check_keywords_alternate(text, keyword_list):\n",
    "    text = text.lower()\n",
    "    # Create a pattern that matches whole words only, for all keywords\n",
    "    pattern = r'\\b(' + '|'.join([re.escape(keyword) for keyword in keyword_list]) + r')\\b'\n",
    "    return int(bool(re.search(pattern, text)))\n",
    "\n",
    "def safety_not_run_thing():\n",
    "    # Convert keywords to lowercase for case-insensitive matching\n",
    "    keywords = {category: [keyword.lower() for keyword in keyword_list] for category, keyword_list in keywords.items()}\n",
    "\n",
    "    # Assuming 'tokenized_text' contains space-separated tokens, it should work well with the modified check_keywords function.\n",
    "    # Just ensure 'tokenized_text' is a string; if it's a list of tokens, you might need to join them first:\n",
    "    # df['tokenized_text_str'] = df['tokenized_text'].apply(' '.join)\n",
    "\n",
    "    for category, keyword_list in keywords.items():\n",
    "        df[category + '_keyword'] = df['tokenized_text'].apply(check_keywords, args=(keyword_list,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tfidf_keyword(df):\n",
    "    # Step 2: TF-IDF Calculation\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)  # Adjust number of features as needed\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['tokenized_text'])\n",
    "\n",
    "    # Step 3. Combine keyword and tfidf features into a single matrix    \n",
    "    # Convert binary keyword matches to a matrix\n",
    "    keyword_features = df[[col for col in df.columns if '_keyword' in col]].to_numpy()\n",
    "    # Combine TF-IDF features with keyword binary indicators\n",
    "    combined_features = np.hstack((tfidf_matrix.toarray(), keyword_features))\n",
    "\n",
    "    # Now `combined_features` is ready for model training, and should be aligned with your labels.\n",
    "    return combined_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training the Classification Model\n",
    "\n",
    "### Next Steps (not-implemented)\n",
    "\n",
    "    Train Your Model: Use the combined_features matrix along with your labels to train and evaluate your classification model.\n",
    "    Evaluation and Refinement: Assess the model's performance and adjust your keyword lists, TF-IDF parameters, or model choice as needed.\n",
    "\n",
    "This approach leverages both the specificity of keyword matching and the nuanced importance scoring of TF-IDF, providing a rich set of features for document classification.\n",
    "\n",
    "- **Splitting Data:** Use your 1000 classified documents as training data. Ensure you have a balanced dataset for the three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y):\n",
    "    # Split the data - 70% for training, 30% for testing; adjust ratios as you see fit\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Model Selection and Training:** Given the textual nature of your task, models like CNN or LSTM could perform well. TensorFlow/Keras will be used here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An initial simple Binary logistic  regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X_train, y_train):\n",
    "\n",
    "    # Initialize the Logistic Regression model\n",
    "    model = LogisticRegression(max_iter=1000)  # Increasing max_iter for convergence\n",
    "\n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Return the trained model\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not using the NN below yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X_train, y_train, X_test, y_test):\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=5000, output_dim=64))  # Adjust according to the TF-IDF feature size\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(3, activation='softmax'))  # Three categories\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Classification of New Documents --- not ready:\n",
    "- **Predicting Categories:** Use the trained model to predict categories for new documents after preprocessing and vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(text):\n",
    "    clean_text = clean_and_tokenize(text)\n",
    "    vectorized_text = vectorizer.transform([clean_text])\n",
    "    prediction = model.predict(vectorized_text)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluation and Iteration not ready\n",
    "\n",
    "- **Evaluation:** Use metrics like accuracy, precision, recall, and F1 score to evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Iteration:** Based on evaluation results, iterate over your model by tuning hyperparameters, trying different models (e.g., BERT for text classification), or using more advanced text vectorization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Scalability and Optimization\n",
    "\n",
    "- Consider parallel processing or distributed computing for preprocessing steps if you face performance bottlenecks.\n",
    "- Explore incremental learning or online learning models if retraining on new data frequently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['link', 'presentation', 'fname', 'cleaned_text', 'tokenized_text'], dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to load df from pickle file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "dataset_path = '../../data/dataset.csv'\n",
    "# DATA_PATH = '/dave/presentations/'\n",
    "DATA_PATH = '/home/mike/Downloads'\n",
    "\n",
    "dfpickle_path = \"/dave/data/df.pkl\"\n",
    "force = False\n",
    "if not os.path.exists(dfpickle_path) or force:\n",
    "    print(\"Warning, is dave mounted?\")\n",
    "else:\n",
    "    print('Going to load df from pickle file')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Careful  with this stored object\n",
    "* it takes nearly 2 hours to generate it\n",
    "* Not that is is the df that include cleaned_text and tokenized_text-- but none of the features generated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(dfpickle_path) and not force:\n",
    "    df = pd.read_pickle(dfpickle_path)\n",
    "else:\n",
    "    df = pd.read_csv(dataset_path, header=0)\n",
    "    df['fname'] = DATA_PATH + df['fname']\n",
    "\n",
    "    tqdm.pandas(desc=\"Processing documents\")\n",
    "    df['cleaned_text'] = df['fname'].progress_apply(pdf_to_text)\n",
    "\n",
    "    df['tokenized_text'] = df['cleaned_text'].progress_apply(clean_and_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next are the features anticipated to identify mix docs around aspect_ratio, text_density and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>presentation</th>\n",
       "      <th>fname</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/134547...</td>\n",
       "      <td>1</td>\n",
       "      <td>/dave/presentations/p23-0016_exhibit1.pdf</td>\n",
       "      <td>Confidential – Not for Reproduction or Distrib...</td>\n",
       "      <td>confidential Reproduction Distribution present...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/129970...</td>\n",
       "      <td>1</td>\n",
       "      <td>/dave/presentations/jan2024deck.pdf</td>\n",
       "      <td>Axos Financial, Inc. \\nInvestor Presentation\\n...</td>\n",
       "      <td>Axos Financial Investor Presentation January N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.sec.gov/files/hamilton-lane-presen...</td>\n",
       "      <td>1</td>\n",
       "      <td>/dave/presentations/hamilton-lane-presentation...</td>\n",
       "      <td>SEC Asset Management Advisory Committee\\nSepte...</td>\n",
       "      <td>SEC Asset Management Advisory Committee Septem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.sec.gov/files/amac-emoney-presenta...</td>\n",
       "      <td>1</td>\n",
       "      <td>/dave/presentations/amac-emoney-presentation-0...</td>\n",
       "      <td>1\\nThe Power of \\nFinancial Planning \\nTechnol...</td>\n",
       "      <td>Power Financial Planning Technology Presentati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.sec.gov/files/amac-kitces-presenta...</td>\n",
       "      <td>1</td>\n",
       "      <td>/dave/presentations/amac-kitces-presentation-0...</td>\n",
       "      <td>1\\nMichael E. Kitces\\nMSFS, MTAX, CFP®, CLU, C...</td>\n",
       "      <td>Michael Kitces MSFS MTAX CFP CLU ChFC RHU REBC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  presentation  \\\n",
       "0  https://www.sec.gov/Archives/edgar/data/134547...             1   \n",
       "1  https://www.sec.gov/Archives/edgar/data/129970...             1   \n",
       "2  https://www.sec.gov/files/hamilton-lane-presen...             1   \n",
       "3  https://www.sec.gov/files/amac-emoney-presenta...             1   \n",
       "4  https://www.sec.gov/files/amac-kitces-presenta...             1   \n",
       "\n",
       "                                               fname  \\\n",
       "0          /dave/presentations/p23-0016_exhibit1.pdf   \n",
       "1                /dave/presentations/jan2024deck.pdf   \n",
       "2  /dave/presentations/hamilton-lane-presentation...   \n",
       "3  /dave/presentations/amac-emoney-presentation-0...   \n",
       "4  /dave/presentations/amac-kitces-presentation-0...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  Confidential – Not for Reproduction or Distrib...   \n",
       "1  Axos Financial, Inc. \\nInvestor Presentation\\n...   \n",
       "2  SEC Asset Management Advisory Committee\\nSepte...   \n",
       "3  1\\nThe Power of \\nFinancial Planning \\nTechnol...   \n",
       "4  1\\nMichael E. Kitces\\nMSFS, MTAX, CFP®, CLU, C...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  confidential Reproduction Distribution present...  \n",
       "1  Axos Financial Investor Presentation January N...  \n",
       "2  SEC Asset Management Advisory Committee Septem...  \n",
       "3  Power Financial Planning Technology Presentati...  \n",
       "4  Michael Kitces MSFS MTAX CFP CLU ChFC RHU REBC...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_data_or_nan(a_list, data):\n",
    "    try:\n",
    "        a_list.append(data)\n",
    "    except Exception as e:\n",
    "        logging.info(e)\n",
    "        a_list.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|          | 0/2000 [22:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7488/2942416902.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error opening PDF {pdf_path}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error opening PDF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/uw/.venvs/matchnames/lib/python3.10/site-packages/fitz/__init__.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[0m\n\u001b[1;32m   2810\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_count2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_count_fz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m             \u001b[0mJM_mupdf_show_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJM_mupdf_show_errors_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: no such file: '/dave/presentations/p23-0016_exhibit1.pdf'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 40\u001b[0m\n\u001b[1;32m     29\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing aspect stuff for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Run your feature extraction functions\u001b[39;00m\n\u001b[1;32m     31\u001b[0m (\n\u001b[1;32m     32\u001b[0m     aspect_ratios,\n\u001b[1;32m     33\u001b[0m     page_count,\n\u001b[1;32m     34\u001b[0m     aspect_threshold,\n\u001b[1;32m     35\u001b[0m     features,\n\u001b[1;32m     36\u001b[0m     num_changes,\n\u001b[1;32m     37\u001b[0m     changes,\n\u001b[1;32m     38\u001b[0m     stats,\n\u001b[1;32m     39\u001b[0m     categories,\n\u001b[0;32m---> 40\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_aspect_ratio_and_mix_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m text_densities \u001b[38;5;241m=\u001b[39m calculate_text_density(pdf_path)\n\u001b[1;32m     43\u001b[0m correlation \u001b[38;5;241m=\u001b[39m correlate_aspect_ratio_text_density(aspect_ratios, text_densities)\n",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m, in \u001b[0;36mcheck_aspect_ratio_and_mix_feature\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m     doc \u001b[38;5;241m=\u001b[39m fitz\u001b[38;5;241m.\u001b[39mopen(pdf_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mlogging\u001b[49m\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening PDF \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening PDF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Returning False as the third value for no significant change\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m, in \u001b[0;36mcheck_aspect_ratio_and_mix_feature\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m     doc \u001b[38;5;241m=\u001b[39m fitz\u001b[38;5;241m.\u001b[39mopen(pdf_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mlogging\u001b[49m\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening PDF \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening PDF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [], \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Returning False as the third value for no significant change\u001b[39;00m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1344\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/uw/.venvs/matchnames/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m/uw/.venvs/matchnames/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assuming `df` is your DataFrame and it has a column `pdf_path` pointing to each PDF file.\n",
    "\n",
    "# Initialize empty lists to store your new features\n",
    "aspect_ratio_means = []\n",
    "aspect_ratio_std = []\n",
    "aspect_ratio_min = []\n",
    "aspect_ratio_max = []\n",
    "page_counts = []\n",
    "aspect_thresholds = []\n",
    "persistent_changes = []\n",
    "persistent_change_counts = []\n",
    "num_ar_changes = []\n",
    "significant_ar_changes = []\n",
    "text_density_means = []\n",
    "text_density_correlations = []\n",
    "categories_lists = []\n",
    "portrait_counts = []\n",
    "landscape_counts = []\n",
    "square_counts = []\n",
    "\n",
    "\n",
    "outliers_counts = []\n",
    "cluster_labels = []\n",
    "cluster_transitions = []\n",
    "cluster_spreads = []\n",
    "\n",
    "for pdf_path in tqdm(df['fname'], desc=\"Processing PDFs\"):\n",
    "    try:\n",
    "        logging.info(f\"Processing aspect stuff for {pdf_path}\")\n",
    "        # Run your feature extraction functions\n",
    "        (\n",
    "            aspect_ratios,\n",
    "            page_count,\n",
    "            aspect_threshold,\n",
    "            features,\n",
    "            num_changes,\n",
    "            changes,\n",
    "            stats,\n",
    "            categories,\n",
    "        ) = check_aspect_ratio_and_mix_feature(pdf_path)\n",
    "        \n",
    "        text_densities = calculate_text_density(pdf_path)\n",
    "        correlation = correlate_aspect_ratio_text_density(aspect_ratios, text_densities)\n",
    "        outliers = detect_outliers_z_score(aspect_ratios)\n",
    "        cluster_label = cluster_aspect_ratios(aspect_ratios)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {pdf_path}: {e}\")\n",
    "        \n",
    "        # For simplicity, let's just use some of the features as examples\n",
    "    append_data_or_nan(aspect_ratio_means, stats['mean'])\n",
    "    append_data_or_nan(aspect_ratio_std, stats['std'])\n",
    "    append_data_or_nan(aspect_ratio_min, stats['min'])\n",
    "    append_data_or_nan(aspect_ratio_max, stats['max'])\n",
    "    append_data_or_nan(page_counts, page_count)\n",
    "    append_data_or_nan(aspect_thresholds, aspect_threshold)\n",
    "    append_data_or_nan(persistent_changes, features['has_persistent_change'])\n",
    "    append_data_or_nan(persistent_change_counts, features['persistent_change_count'])\n",
    "    append_data_or_nan(num_ar_changes, num_changes)\n",
    "    append_data_or_nan(significant_ar_changes, changes)\n",
    "    append_data_or_nan(text_density_means, np.mean(text_densities))\n",
    "    append_data_or_nan(text_density_correlations, correlation)\n",
    "    append_data_or_nan(categories_lists, categories)  # This one is a bit tricky as it's a list. Might aggregate or process further.\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    append_data_or_nan(outliers_counts, len(outliers) )  # Assuming aspect ratios are recalculated within the function\n",
    "    append_data_or_nan(cluster_labels,cluster_label)\n",
    "    \n",
    "portrait_counts = [cats.count('portrait') if isinstance(cats, list) else 0 for cats in categories_lists]\n",
    "landscape_counts = [cats.count('landscape') if isinstance(cats, list) else 0 for cats in categories_lists]\n",
    "square_counts = [cats.count('square') if isinstance(cats, list) else 0 for cats in categories_lists]\n",
    "cluster_features = calculate_features(cluster_labels)\n",
    "cluster_transitions =[x[0] for x in cluster_features]\n",
    "cluster_spreads = [x[1] for x in cluster_features]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (0) does not match length of index (2000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now add these lists as columns to your DataFrame\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maspect_ratio_means\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m aspect_ratio_means\n\u001b[1;32m      3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maspect_ratio_std\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m  aspect_ratio_std\n\u001b[1;32m      4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maspect_ratio_min\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m aspect_ratio_min\n",
      "File \u001b[0;32m/uw/.venvs/matchnames/lib/python3.10/site-packages/pandas/core/frame.py:4299\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4298\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4299\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/uw/.venvs/matchnames/lib/python3.10/site-packages/pandas/core/frame.py:4512\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4502\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4504\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4505\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4510\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4511\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4512\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4515\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4516\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4517\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4518\u001b[0m     ):\n\u001b[1;32m   4519\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4520\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m/uw/.venvs/matchnames/lib/python3.10/site-packages/pandas/core/frame.py:5253\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5253\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5254\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5256\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[1;32m   5257\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[1;32m   5261\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[0;32m/uw/.venvs/matchnames/lib/python3.10/site-packages/pandas/core/common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (0) does not match length of index (2000)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Now add these lists as columns to your DataFrame\n",
    "df['aspect_ratio_means'] = aspect_ratio_means\n",
    "df['aspect_ratio_std'] =  aspect_ratio_std\n",
    "df['aspect_ratio_min'] = aspect_ratio_min\n",
    "df['aspect_ratio_max'] = aspect_ratio_max\n",
    "df['page_counts'] = page_counts\n",
    "df['aspect_thresholds'] = aspect_thresholds\n",
    "df['persistent_changes'] = persistent_changes\n",
    "df['persistent_change_counts'] = persistent_change_counts\n",
    "df['num_ar_changes'] = num_ar_changes\n",
    "df['significant_ar_changes'] = significant_ar_changes\n",
    "df['text_density_means'] = text_density_means\n",
    "df['text_density_correlations'] = text_density_correlations\n",
    "df['portrait_count'] = portrait_counts\n",
    "df['landscape_count'] = landscape_counts\n",
    "df['square_count'] = square_counts\n",
    "# For categories, you might need to process the lists into something usable as a feature. Example below.\n",
    "df['outliers_counts'] = outliers_counts\n",
    "df['cluster_transitions'] = cluster_transitions\n",
    "df['cluster_spreads'] = cluster_spreads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans_labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uw/.venvs/matchnames/lib/python3.10/site-packages/sklearn/base.py:1474: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "kmeans_labels = cluster_aspect_ratios(aspect_ratios)\n",
    "print(f\"kmeans_labels:  {kmeans_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outliers: []\n"
     ]
    }
   ],
   "source": [
    "outliers = detect_outliers_z_score(aspect_ratios)\n",
    "print(f\"outliers: {outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_densities: [9.696639598600383e-05, 0.0016071664355978082, 0.00184029840892586, 0.0006148082128474285, 0.0001258500033009837, 0.0006024295239981514, 0.0006189344424638542, 0.0011017033075856606, 0.000662259853436324, 0.000662259853436324, 0.0008252459232851389, 0.00089126559714795, 0.0003816762395193768, 0.0006395655905459827, 0.000445632798573975, 0.0005652934574503202, 0.0005508516537928303, 0.0004002442727932924, 0.0008231828084769261, 0.00037136066547831253, 0.000445632798573975, 0.0006416287053541955, 0.0007241532976827095, 0.0002867729583415858, 0.00031153033604013996, 0.0004930844391628705, 0.0006333762461213442, 0.0005425991945599789, 0.0006313131313131314, 0.0003362877137386941, 0.00024757377698554167, 0.0003300983693140556, 0.0004023073876015052, 0.0003878655839440153, 0.0005570409982174688]\n",
      "correlation: -2.8941088484809674e-16\n"
     ]
    }
   ],
   "source": [
    "text_densities = calculate_text_density(testit)\n",
    "correlation = correlate_aspect_ratio_text_density(aspect_ratios, text_densities)\n",
    "print(f\"text_densities: {text_densities}\")\n",
    "print(f\"correlation: {correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {\n",
    "    \"financial_terms\": ['financial', 'investment', 'share price', 'financial metrics', 'investment strategy'],\n",
    "    \"legal_statements\": ['confidentiality statement', 'legal disclaimer', 'disclosure statement', 'proprietary information', 'intellectual property'],\n",
    "    \"company_info\": ['company overview', 'company analysis', 'business model', 'company performance'],\n",
    "    \"presentation_content\": ['visual aids', 'data charts', 'case studies', 'comparative analysis'],\n",
    "    \"company_targets\": ['sales targets', 'company targets', 'performance targets'],\n",
    "    \"financial_discussions\": ['financial figures', 'financial projections', 'financial results', 'financial language'],\n",
    "    \"regulatory_references\": ['SEC filings', 'regulatory filings', 'external entities', 'lawsuits'],\n",
    "    \"detail_descriptions\": ['loan details', 'product details', 'research and development', 'financial details'],\n",
    "    \"company_specific\": ['company specific', 'industry specific', 'company-specific analysis', 'specific company focus']\n",
    "    # \"Other Clusters\" category is omitted since it's broad and without specific keywords\n",
    "}\n",
    "\n",
    "\n",
    "# Apply keyword matching\n",
    "for category, keyword_list in keywords.items():\n",
    "    df[category + '_keyword'] = df['tokenized_text'].apply(check_keywords, args=(keyword_list,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = combine_tfidf_keyword(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(features, df['presentation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='binary'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='binary'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average='binary'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame `df_test` corresponding to your test dataset\n",
    "# And it includes a column 'doc_id' or similar that uniquely identifies each document\n",
    "# If you don't have such a DataFrame, you can create it from `X_test` and `y_test`\n",
    "\n",
    "\n",
    "type(X_test), type(y_test)\n",
    "# df_test = pd.DataFrame({'doc_id': X_test.index, 'text': X_test, 'label': y_test})\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First, ensure `X_test` retains its index after splitting so you can merge based on index\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'True Label': y_test,\n",
    "    'Predicted Label': y_pred\n",
    "})\n",
    "\n",
    "# If `X_test` and `y_test` don't automatically align, you might need to reset the index\n",
    "# misclassified_df = misclassified_df.reset_index()\n",
    "\n",
    "# Add a column to indicate whether each prediction is correct\n",
    "misclassified_df['Correctly Classified'] = misclassified_df['True Label'] == misclassified_df['Predicted Label']\n",
    "\n",
    "# Filter the DataFrame to only include misclassified documents\n",
    "misclassified_docs = misclassified_df[~misclassified_df['Correctly Classified']]\n",
    "\n",
    "# Optionally, join with the original DataFrame (df) to include text or other identifying information\n",
    "# This step requires that `df` and `misclassified_docs` can be aligned by index or a unique identifier\n",
    "# Example:\n",
    "# misclassified_docs = misclassified_docs.join(df[['doc_id', 'text']], how='left')\n",
    "\n",
    "misclassified_docs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matchnames",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
